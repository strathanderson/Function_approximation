{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import inspect\n",
    "import os\n",
    "np.random.seed(0)\n",
    "\n",
    "#Activation functions\n",
    "\n",
    "def sin_function(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "#Function to approximate\n",
    "def f(x):\n",
    "    return np.sin(3*np.pi*x + 3*np.pi/20) * np.cos(2*np.pi*x + np.pi/10) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the mass matrix - M\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, n):\n",
    "    M = np.zeros([n, n])\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i+1,j+1,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single basis function - Neural Network\n",
    "def single_neural_net(x,weights,biases,i, sigma): \n",
    "\n",
    "    return sigma(weights[i]*x+biases[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basis function multiplication helper for quadrature\n",
    "def double_neural_net(x,weights,biases,i,j, sigma): \n",
    "\n",
    "    return single_neural_net(x,weights,biases,i,sigma)*single_neural_net(x,weights,biases,j,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias helper function for quadrature\n",
    "def b_neural_net(x, weights, biases,i, sigma,f):\n",
    "    return single_neural_net(x,weights,biases,i, sigma)*f(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quadrature for the bias matrix - B\n",
    "def b_matrix(func,sigma, weights, biases,f, neurons):\n",
    "    b = np.zeros([neurons, 1])\n",
    "    for i in range(0, neurons):\n",
    "            b[i] = quad(func, 0, 1, args=(weights, biases,i,sigma,f), limit=1000)[0]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the approximation vector\n",
    "def u_nn(x, weights, biases, sigma,collocation, neurons, a):\n",
    "    u = np.zeros(collocation)\n",
    "    \n",
    "    for j in range(0,collocation):\n",
    "        for i in range(0,neurons):\n",
    "            \n",
    "            u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n",
    "        \n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for eigenvalues and eigenvectors\n",
    "from scipy.linalg import eig\n",
    "def get_eig(M):\n",
    "    D,V = eig(M, left=False, right=True)\n",
    "\n",
    "\n",
    "    idx = D.argsort()[::-1]   \n",
    "    D = D[idx]\n",
    "    V = V[:,idx]\n",
    "\n",
    "    D = np.flip(D)\n",
    "    V = np.fliplr(V)\n",
    "\n",
    "    return D,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate L2 loss\n",
    "def l2_loss(f_x, u_nn_result):\n",
    "    return np.linalg.norm(f_x - u_nn_result)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the Phi matrix, made up of n basis functions\n",
    "def phi_matrix(x,weights,biases,sigma,neurons,collocation):\n",
    "    phi = np.zeros((collocation,neurons))\n",
    "    for i in range(0,neurons):\n",
    "        for j in range(0,collocation):\n",
    "\n",
    "            phi[j,i] = sigma(weights[i]*x[j]+biases[i])\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=False):\n",
    "    phi = phi_matrix(x,weights,biases,sigma,neurons,collocation)\n",
    "    f_func = f(x)\n",
    "    a = np.linalg.pinv(phi)@f_func\n",
    "\n",
    "    u = u_nn(x, weights, biases, sigma, collocation,neurons, a)\n",
    "\n",
    "    loss = l2_loss(f_func, u)\n",
    "    if verbose:\n",
    "        print(\"=\"*20)\n",
    "        print(\"Sigma:\", sigma.__name__)\n",
    "        print(\"Number of collocation points:\", collocation)\n",
    "        print(\"Number of neurons:\", neurons)\n",
    "        print(\"L2 Loss:\", loss)\n",
    "        print(\"=\"*20)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_list(data, list_name, sheet_name, directory='/mnt/c/Git_Repos/Function_approximation/Results', columns=[\"Loss\", \"# of Collocation\", \"# of Neurons\", \"Activation Function\", \"R - Sample Range\"]):\n",
    "    file_name = f\"{list_name}.xlsx\"\n",
    "    full_path = os.path.join(directory, file_name) if directory else file_name\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    if os.path.exists(full_path):\n",
    "        try:\n",
    "            # If the file exists, load the workbook and append the data to a new sheet\n",
    "            with pd.ExcelWriter(full_path, engine='openpyxl', mode='a') as writer:\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except ValueError as e:\n",
    "            # If the sheet already exists, print a message and continue\n",
    "            print(f\"Sheet name '{sheet_name}' already exists in the file '{file_name}'\")\n",
    "    else:\n",
    "        # If the file does not exist, create a new one\n",
    "        df.to_excel(full_path, index=False, sheet_name=sheet_name)\n",
    "\n",
    "    print(f\"Data exported to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_loss_values(neurons, collocations, sigmas, Rs,sheet_name,xmin=0,xmax=1,verbose=False):\n",
    "    loss_values = []\n",
    "    #np.random.seed(0)  # Set a seed for reproducibility\n",
    "    for sigma in sigmas:\n",
    "        for neuron in neurons:\n",
    "            for collocation in collocations:\n",
    "                for R in Rs:\n",
    "                    x = np.linspace(xmin, xmax, collocation)\n",
    "                    weights = np.random.uniform(-R, R, neuron)\n",
    "                    biases = np.random.uniform(-R, R, neuron)\n",
    "                    loss = evaluate_func_approx(x, f, weights, biases, sigma, neuron, collocation, verbose=verbose)\n",
    "                    loss_values.append((loss, collocation, neuron, sigma.__name__,R))  # Add activation function name\n",
    "    export_list(loss_values, \"Func_Approx_Method_Results\",sheet_name)\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_collocation(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by collocation point, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each collocation point\n",
    "    collocation_loss_data = {collocation: [] for _, collocation, _, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (neuron, loss, activation_func_name, R) tuples for each collocation point\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        collocation_loss_data[collocation].append((neuron, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the collocation points and print the relevant data\n",
    "        for collocation, data in collocation_loss_data.items():\n",
    "            print(f\"# of Collocation points: {collocation}\")\n",
    "            \n",
    "            # Print the neurons, loss values, and R for the current activation function\n",
    "            for neuron, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Neurons: {neuron}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_neuron(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by neuron, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each neuron\n",
    "    neuron_loss_data = {neuron: [] for _, _, neuron, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, loss, activation_func_name, R) tuples for each neuron\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        neuron_loss_data[neuron].append((collocation, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the neurons and print the relevant data\n",
    "        for neuron, data in neuron_loss_data.items():\n",
    "            print(f\"# of Neurons: {neuron}\")\n",
    "            \n",
    "            # Print the collocation points, loss values, and R for the current activation function\n",
    "            for collocation, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_R(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by the range R, activation function, collocation points, and neurons.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each R value\n",
    "    R_loss_data = {R: [] for _, _, _, _, R in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, neuron, loss, activation_func_name) tuples for each R value\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        R_loss_data[R].append((collocation, neuron, loss, activation_func_name))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the R values and print the relevant data\n",
    "        for R, data in R_loss_data.items():\n",
    "            print(f\"Range R: {R}\")\n",
    "            \n",
    "            # Print the collocation points, neurons, and loss values for the current activation function\n",
    "            for collocation, neuron, loss, act_func_name in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, # of Neurons: {neuron}, Loss: {loss:.10e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Sigma: sin_function\n",
      "Number of collocation points: 100\n",
      "Number of neurons: 50\n",
      "L2 Loss: 0.08512083411934636\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "collocation = 100\n",
    "neurons = 50\n",
    "sigma = sin_function\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-1,1,neurons)\n",
    "biases = np.random.uniform(-1,1,neurons)\n",
    "test = evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Increasing #of neurons\n",
    "Keeping collocations fixed at 50, and using ReLU, tanh, and sin as activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Increasing Neurons' already exists in the file 'Func_Approx_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Func_Approx_Method_Results.xlsx\n"
     ]
    }
   ],
   "source": [
    "neurons = [5,10,20,30,40,50,100]\n",
    "sigmas = [relu,tanh,sin_function]\n",
    "collocations = [50]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Increasing Neurons\",verbose=False)\n",
    "#print_loss_by_R(loss_values)\n",
    "#loss_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - Increasing Collocation points\n",
    "Keeping neurons fixed at 20, and the same activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Increasing Collocation' already exists in the file 'Func_Approx_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Func_Approx_Method_Results.xlsx\n"
     ]
    }
   ],
   "source": [
    "neurons = [20]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [10,50,100,200,500]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Increasing Collocation\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_neuron(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 - Increasing the sampling range R\n",
    "Keeping neurons and collocations fixed, increasing R to increase chance that w_i and b_i are further away from w_j and b_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Increasing Sampling Range' already exists in the file 'Func_Approx_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Func_Approx_Method_Results.xlsx\n"
     ]
    }
   ],
   "source": [
    "neurons = [30]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [50]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Increasing Sampling Range\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Full_Test' already exists in the file 'Func_Approx_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Func_Approx_Method_Results.xlsx\n"
     ]
    }
   ],
   "source": [
    "neurons = [10,20,30,50,100]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [5,10,30,50,100]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Full_Test\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "from numpy.linalg import cond\n",
    "from scipy.linalg import eig\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, neurons):\n",
    "    M = np.zeros([neurons, neurons])\n",
    "    \n",
    "    for i in range(0, neurons):\n",
    "        for j in range(0, neurons):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i,j,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41674024, -0.11257135, -0.31010403,  0.35565366,  0.4666178 ,\n",
       "        -0.06564373,  0.03690916,  0.11609082,  0.05956116,  0.08696524,\n",
       "        -0.05841472, -0.03942721,  0.03068472,  0.41779842,  0.42155246,\n",
       "         0.41778513,  0.49071799,  0.01929366, -0.0371087 ,  0.12310746],\n",
       "       [-0.11257135,  0.54251744, -0.08207161, -0.03477159,  0.01379782,\n",
       "        -0.09330023,  0.24590998, -0.33755141, -0.15523795, -0.45037547,\n",
       "         0.50608706,  0.26608715, -0.224019  , -0.08302562, -0.08441021,\n",
       "         0.06410157, -0.12016964, -0.02202749,  0.44658847,  0.14873242],\n",
       "       [-0.31010403, -0.08207161,  0.51284027, -0.4786496 , -0.46479543,\n",
       "         0.39717221,  0.04032458, -0.02229856,  0.33350763,  0.1170935 ,\n",
       "        -0.19978534, -0.0905107 ,  0.09716657, -0.46171405, -0.20921444,\n",
       "        -0.40793953, -0.33597419,  0.10967025, -0.26968949, -0.45816767],\n",
       "       [ 0.35565366, -0.03477159, -0.4786496 ,  0.47943313,  0.47123124,\n",
       "        -0.34157639, -0.08520479,  0.09443258, -0.25959694, -0.02028395,\n",
       "         0.07594028,  0.03955306, -0.05372609,  0.48341149,  0.26025645,\n",
       "         0.40326798,  0.39007978, -0.08961236,  0.15131131,  0.39214761],\n",
       "       [ 0.4666178 ,  0.01379782, -0.46479543,  0.47123124,  0.59248377,\n",
       "        -0.1910497 ,  0.10642992,  0.03584318, -0.05336521, -0.01320804,\n",
       "         0.10060883,  0.00293905, -0.00451633,  0.5158159 ,  0.45395729,\n",
       "         0.54287754,  0.54550445, -0.0156395 ,  0.12499043,  0.27739249],\n",
       "       [-0.06564373, -0.09330023,  0.39717221, -0.34157639, -0.1910497 ,\n",
       "         0.4768144 ,  0.2009732 , -0.02870677,  0.50637555,  0.16451974,\n",
       "        -0.20572437, -0.14534909,  0.15787157, -0.27159115,  0.09613707,\n",
       "        -0.14008992, -0.03045982,  0.16912854, -0.30489934, -0.48942515],\n",
       "       [ 0.03690916,  0.24590998,  0.04032458, -0.08520479,  0.10642992,\n",
       "         0.2009732 ,  0.50076916, -0.39287349,  0.29556855, -0.02214806,\n",
       "         0.19284904, -0.17121066,  0.21097621, -0.05903633,  0.1717343 ,\n",
       "         0.16687667,  0.08531431,  0.09035363,  0.06914692, -0.14334415],\n",
       "       [ 0.11609082, -0.33755141, -0.02229856,  0.09443258,  0.03584318,\n",
       "        -0.02870677, -0.39287349,  0.51541347, -0.06138259,  0.1087455 ,\n",
       "        -0.27750416,  0.13190991, -0.17303064,  0.12214107,  0.06456372,\n",
       "        -0.01700945,  0.1192403 , -0.00608769, -0.18011972,  0.00240755],\n",
       "       [ 0.05956116, -0.15523795,  0.33350763, -0.25959694, -0.05336521,\n",
       "         0.50637555,  0.29556855, -0.06138259,  0.60480499,  0.26967515,\n",
       "        -0.25856744, -0.26486675,  0.27793458, -0.16415635,  0.24276299,\n",
       "        -0.00958161,  0.12271616,  0.19067918, -0.37153504, -0.50249871],\n",
       "       [ 0.08696524, -0.45037547,  0.1170935 , -0.02028395, -0.01320804,\n",
       "         0.16451974, -0.02214806,  0.1087455 ,  0.26967515,  0.48206142,\n",
       "        -0.44074327, -0.40374169,  0.3791624 ,  0.02953666,  0.10409645,\n",
       "        -0.03685532,  0.10409458,  0.04727526, -0.43633559, -0.20696632],\n",
       "       [-0.05841472,  0.50608706, -0.19978534,  0.07594028,  0.10060883,\n",
       "        -0.20572437,  0.19284904, -0.27750416, -0.25856744, -0.44074327,\n",
       "         0.51701308,  0.27040966, -0.23647618,  0.01782015, -0.0660659 ,\n",
       "         0.13422301, -0.06687858, -0.05842352,  0.48886204,  0.26798382],\n",
       "       [-0.03942721,  0.26608715, -0.0905107 ,  0.03955306,  0.00293905,\n",
       "        -0.14534909, -0.17121066,  0.13190991, -0.26486675, -0.40374169,\n",
       "         0.27040966,  0.44633702, -0.44155719,  0.00846862, -0.07665436,\n",
       "         0.00114437, -0.05552566, -0.04097565,  0.30783232,  0.16794755],\n",
       "       [ 0.03068472, -0.224019  ,  0.09716657, -0.05372609, -0.00451633,\n",
       "         0.15787157,  0.21097621, -0.17303064,  0.27793458,  0.3791624 ,\n",
       "        -0.23647618, -0.44155719,  0.44142724, -0.02333314,  0.07797042,\n",
       "         0.00387138,  0.0484916 ,  0.04645976, -0.2850718 , -0.17569094],\n",
       "       [ 0.41779842, -0.08302562, -0.46171405,  0.48341149,  0.5158159 ,\n",
       "        -0.27159115, -0.05903633,  0.12214107, -0.16415635,  0.02953666,\n",
       "         0.01782015,  0.00846862, -0.02333314,  0.51090172,  0.34819067,\n",
       "         0.44605689,  0.47033415, -0.05796275,  0.08287154,  0.32767379],\n",
       "       [ 0.42155246, -0.08441021, -0.20921444,  0.26025645,  0.45395729,\n",
       "         0.09613707,  0.1717343 ,  0.06456372,  0.24276299,  0.10409645,\n",
       "        -0.0660659 , -0.07665436,  0.07797042,  0.34819067,  0.49638888,\n",
       "         0.42880878,  0.51729768,  0.08346378, -0.09068478, -0.0266205 ],\n",
       "       [ 0.41778513,  0.06410157, -0.40793953,  0.40326798,  0.54287754,\n",
       "        -0.14008992,  0.16687667, -0.01700945, -0.00958161, -0.03685532,\n",
       "         0.13422301,  0.00114437,  0.00387138,  0.44605689,  0.42880878,\n",
       "         0.50938554,  0.49549808,  0.00066402,  0.13778607,  0.22797068],\n",
       "       [ 0.49071799, -0.12016964, -0.33597419,  0.39007978,  0.54550445,\n",
       "        -0.03045982,  0.08531431,  0.1192403 ,  0.12271616,  0.10409458,\n",
       "        -0.06687858, -0.05552566,  0.0484916 ,  0.47033415,  0.51729768,\n",
       "         0.49549808,  0.58411438,  0.04139009, -0.05573668,  0.10213522],\n",
       "       [ 0.01929366, -0.02202749,  0.10967025, -0.08961236, -0.0156395 ,\n",
       "         0.16912854,  0.09035363, -0.00608769,  0.19067918,  0.04727526,\n",
       "        -0.05842352, -0.04097565,  0.04645976, -0.05796275,  0.08346378,\n",
       "         0.00066402,  0.04139009,  0.06603309, -0.09565607, -0.16363977],\n",
       "       [-0.0371087 ,  0.44658847, -0.26968949,  0.15131131,  0.12499043,\n",
       "        -0.30489934,  0.06914692, -0.18011972, -0.37153504, -0.43633559,\n",
       "         0.48886204,  0.30783232, -0.2850718 ,  0.08287154, -0.09068478,\n",
       "         0.13778607, -0.05573668, -0.09565607,  0.49994993,  0.35922251],\n",
       "       [ 0.12310746,  0.14873242, -0.45816767,  0.39214761,  0.27739249,\n",
       "        -0.48942515, -0.14334415,  0.00240755, -0.50249871, -0.20696632,\n",
       "         0.26798382,  0.16794755, -0.17569094,  0.32767379, -0.0266205 ,\n",
       "         0.22797068,  0.10213522, -0.16363977,  0.35922251,  0.52235304]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons = 20\n",
    "R=10\n",
    "collocation = 10\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-R,R,neurons)\n",
    "biases = np.random.uniform(-R,R,neurons)\n",
    "sigma = sin_function\n",
    "M = mass_matrix(double_neural_net,sigma, weights, biases, neurons)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.473292981429498e-185"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_m = np.linalg.det(M)\n",
    "det_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0057548795552702e+19"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_m = np.linalg.cond(M)\n",
    "cond_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b_matrix(b_neural_net,sigma,weights, biases,f,neurons)\n",
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linalg.solve(M,b)\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23215/1981966750.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n"
     ]
    }
   ],
   "source": [
    "u = u_nn(x,weights,biases,sigma,collocation,neurons,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.43918305, 2.53021268, 1.92421898, 2.33745398, 2.99751064,\n",
       "       2.43020355, 1.90419358, 2.47027579, 2.50522261, 1.57880669])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Loss: 0.000173335145905158\n"
     ]
    }
   ],
   "source": [
    "loss = l2_loss(f(x), u)\n",
    "print(\"L2 Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6818c013192d840514133cc15ea0bc15bb95d666290febd40d542c4e37363947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
