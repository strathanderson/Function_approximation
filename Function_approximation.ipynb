{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "#Activation functions\n",
    "\n",
    "def sin_function(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "#Function to approximate\n",
    "def f(x):\n",
    "    return np.sin(3*np.pi*x + 3*np.pi/20) * np.cos(2*np.pi*x + np.pi/10) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the mass matrix - M\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, n):\n",
    "    M = np.zeros([n, n])\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i+1,j+1,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single basis function - Neural Network\n",
    "def single_neural_net(x,weights,biases,i, sigma): \n",
    "\n",
    "    return sigma(weights[i]*x+biases[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basis function multiplication helper for quadrature\n",
    "def double_neural_net(x,weights,biases,i,j, sigma): \n",
    "\n",
    "    return single_neural_net(x,weights,biases,i,sigma)*single_neural_net(x,weights,biases,j,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias helper function for quadrature\n",
    "def b_neural_net(x, weights, biases,i, sigma,f):\n",
    "    return single_neural_net(x,weights,biases,i, sigma)*f(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quadrature for the bias matrix - B\n",
    "def b_matrix(func,sigma, weights, biases,f, neurons):\n",
    "    b = np.zeros([neurons, 1])\n",
    "    for i in range(0, neurons):\n",
    "            b[i] = quad(func, 0, 1, args=(weights, biases,i,sigma,f), limit=1000)[0]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the approximation vector\n",
    "def u_nn(x, weights, biases, sigma,collocation, neurons, a):\n",
    "    u = np.zeros(collocation)\n",
    "    \n",
    "    for j in range(0,collocation):\n",
    "        for i in range(0,neurons):\n",
    "            \n",
    "            u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n",
    "        \n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for eigenvalues and eigenvectors\n",
    "from scipy.linalg import eig\n",
    "def get_eig(M):\n",
    "    D,V = eig(M, left=False, right=True)\n",
    "\n",
    "\n",
    "    idx = D.argsort()[::-1]   \n",
    "    D = D[idx]\n",
    "    V = V[:,idx]\n",
    "\n",
    "    D = np.flip(D)\n",
    "    V = np.fliplr(V)\n",
    "\n",
    "    return D,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate L2 loss\n",
    "def l2_loss(f_x, u_nn_result):\n",
    "    return np.linalg.norm(f_x - u_nn_result)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the Phi matrix, made up of n basis functions\n",
    "def phi_matrix(x,weights,biases,sigma,neurons,collocation):\n",
    "    phi = np.zeros((collocation,neurons))\n",
    "    for i in range(0,neurons):\n",
    "        for j in range(0,collocation):\n",
    "\n",
    "            phi[j,i] = sigma(weights[i]*x[j]+biases[i])\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=False):\n",
    "    phi = phi_matrix(x,weights,biases,sigma,neurons,collocation)\n",
    "    f_func = f(x)\n",
    "    a = np.linalg.pinv(phi)@f_func\n",
    "\n",
    "    u = u_nn(x, weights, biases, sigma, collocation,neurons, a)\n",
    "\n",
    "    loss = l2_loss(f_func, u)\n",
    "    if verbose:\n",
    "        print(\"=\"*20)\n",
    "        print(\"Sigma:\", sigma.__name__)\n",
    "        print(\"Number of collocation points:\", collocation)\n",
    "        print(\"Number of neurons:\", neurons)\n",
    "        print(\"L2 Loss:\", loss)\n",
    "        print(\"=\"*20)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_loss_values(neurons, collocations, sigmas, Rs,xmin=0,xmax=1,verbose=False):\n",
    "    loss_values = []\n",
    "    #np.random.seed(0)  # Set a seed for reproducibility\n",
    "    for sigma in sigmas:\n",
    "        for neuron in neurons:\n",
    "            for collocation in collocations:\n",
    "                for R in Rs:\n",
    "                    x = np.linspace(xmin, xmax, collocation)\n",
    "                    weights = np.random.uniform(-R, R, neuron)\n",
    "                    biases = np.random.uniform(-R, R, neuron)\n",
    "                    loss = evaluate_func_approx(x, f, weights, biases, sigma, neuron, collocation, verbose=verbose)\n",
    "                    loss_values.append((loss, collocation, neuron, sigma.__name__,R))  # Add activation function name\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_collocation(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by collocation point, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each collocation point\n",
    "    collocation_loss_data = {collocation: [] for _, collocation, _, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (neuron, loss, activation_func_name, R) tuples for each collocation point\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        collocation_loss_data[collocation].append((neuron, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the collocation points and print the relevant data\n",
    "        for collocation, data in collocation_loss_data.items():\n",
    "            print(f\"# of Collocation points: {collocation}\")\n",
    "            \n",
    "            # Print the neurons, loss values, and R for the current activation function\n",
    "            for neuron, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Neurons: {neuron}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_neuron(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by neuron, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each neuron\n",
    "    neuron_loss_data = {neuron: [] for _, _, neuron, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, loss, activation_func_name, R) tuples for each neuron\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        neuron_loss_data[neuron].append((collocation, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the neurons and print the relevant data\n",
    "        for neuron, data in neuron_loss_data.items():\n",
    "            print(f\"# of Neurons: {neuron}\")\n",
    "            \n",
    "            # Print the collocation points, loss values, and R for the current activation function\n",
    "            for collocation, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_R(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by the range R, activation function, collocation points, and neurons.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each R value\n",
    "    R_loss_data = {R: [] for _, _, _, _, R in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, neuron, loss, activation_func_name) tuples for each R value\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        R_loss_data[R].append((collocation, neuron, loss, activation_func_name))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the R values and print the relevant data\n",
    "        for R, data in R_loss_data.items():\n",
    "            print(f\"Range R: {R}\")\n",
    "            \n",
    "            # Print the collocation points, neurons, and loss values for the current activation function\n",
    "            for collocation, neuron, loss, act_func_name in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, # of Neurons: {neuron}, Loss: {loss:.10e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Sigma: sin_function\n",
      "Number of collocation points: 100\n",
      "Number of neurons: 50\n",
      "L2 Loss: 0.08512083411934636\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "collocation = 100\n",
    "neurons = 50\n",
    "sigma = sin_function\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-1,1,neurons)\n",
    "biases = np.random.uniform(-1,1,neurons)\n",
    "test = evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Increasing #of neurons\n",
    "Keeping collocations fixed at 50, and using ReLU, tanh, and sin as activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [5,10,20,30,40,50,100]\n",
    "sigmas = [relu,tanh,sin_function]\n",
    "collocations = [50]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)\n",
    "#print_loss_by_R(loss_values)\n",
    "#loss_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - Increasing Collocation points\n",
    "Keeping neurons fixed at 20, and the same activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [20]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [10,50,100,200,500]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_neuron(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 - Increasing the sampling range R\n",
    "Keeping neurons and collocations fixed, increasing R to increase chance that w_i and b_i are further away from w_j and b_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [30]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [50]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [10,20,30,50,100]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [5,10,30,50,100]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "from numpy.linalg import cond\n",
    "from scipy.linalg import eig\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, neurons):\n",
    "    M = np.zeros([neurons, neurons])\n",
    "    \n",
    "    for i in range(0, neurons):\n",
    "        for j in range(0, neurons):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i,j,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.03629233e-01, -2.03816594e-01, -3.56864545e-01,\n",
       "         1.99093072e-01,  2.61573437e-01,  3.46475774e-02,\n",
       "         4.14404076e-01, -3.17610211e-01, -4.78769183e-02,\n",
       "        -1.02233116e-01, -4.27505846e-01,  1.54976221e-01,\n",
       "        -2.88827610e-01,  1.89903947e-01,  3.33870699e-01,\n",
       "         1.86772591e-01, -3.08547512e-01, -4.90171689e-02,\n",
       "         1.23214863e-01, -4.23509637e-01],\n",
       "       [-2.03816594e-01,  4.05264567e-01,  3.09535112e-01,\n",
       "         6.43438507e-02,  3.58463897e-02, -2.89367686e-02,\n",
       "        -2.60687488e-01, -1.50355219e-01, -1.12226779e-01,\n",
       "        -1.86464235e-01, -1.78610892e-02, -2.13823067e-01,\n",
       "         9.85981776e-02, -3.54340588e-01, -3.77933731e-01,\n",
       "        -2.41244454e-02,  1.28263651e-01,  3.58881489e-01,\n",
       "        -2.63960405e-01,  3.13495417e-01],\n",
       "       [-3.56864545e-01,  3.09535112e-01,  5.69907119e-01,\n",
       "         1.06547375e-01,  3.38108497e-02, -8.56365887e-02,\n",
       "        -6.34109998e-01, -8.85802379e-02,  1.41683173e-02,\n",
       "        -3.04206515e-01,  3.68463466e-02, -2.22111984e-02,\n",
       "         3.02193743e-01, -8.38757854e-02, -3.60630699e-01,\n",
       "        -2.01829005e-02,  6.52658956e-02,  2.64017607e-02,\n",
       "        -4.98425541e-01,  2.70487406e-01],\n",
       "       [ 1.99093072e-01,  6.43438507e-02,  1.06547375e-01,\n",
       "         2.75278646e-01,  2.78514300e-01, -3.30143323e-02,\n",
       "        -9.59516929e-02, -3.73058400e-01, -1.52111383e-02,\n",
       "        -3.37971274e-01, -3.75780499e-01,  9.57344814e-02,\n",
       "        -4.39892610e-02,  8.49578415e-02,  2.15091577e-02,\n",
       "         1.35513611e-01, -2.17246533e-01, -5.20024225e-03,\n",
       "        -2.73346481e-01, -1.74735091e-01],\n",
       "       [ 2.61573437e-01,  3.58463897e-02,  3.38108497e-02,\n",
       "         2.78514300e-01,  2.92634833e-01, -2.23862551e-02,\n",
       "        -9.77731267e-03, -3.90053899e-01, -2.51713277e-02,\n",
       "        -3.18397214e-01, -4.07562341e-01,  9.81085341e-02,\n",
       "        -9.20077572e-02,  8.95718924e-02,  6.53183471e-02,\n",
       "         1.48567189e-01, -2.37976824e-01,  5.63472913e-03,\n",
       "        -2.21137180e-01, -2.16957566e-01],\n",
       "       [ 3.46475774e-02, -2.89367686e-02, -8.56365887e-02,\n",
       "        -3.30143323e-02, -2.23862551e-02,  1.53976238e-02,\n",
       "         1.00214429e-01,  2.98968958e-02, -9.34062957e-03,\n",
       "         6.33343035e-02,  1.77521153e-02, -1.79191714e-02,\n",
       "        -4.35993293e-02, -1.69535028e-02,  3.42205183e-02,\n",
       "        -7.34253379e-03,  1.26867780e-02,  2.16304553e-02,\n",
       "         8.77638499e-02, -1.25871882e-02],\n",
       "       [ 4.14404076e-01, -2.60687488e-01, -6.34109998e-01,\n",
       "        -9.59516929e-02, -9.77731267e-03,  1.00214429e-01,\n",
       "         7.37232235e-01,  3.51303172e-02, -5.28385248e-02,\n",
       "         3.08701116e-01, -8.39132395e-02, -3.73348365e-02,\n",
       "        -3.67686478e-01, -5.14407603e-03,  3.43969692e-01,\n",
       "         2.60122862e-02, -5.66293668e-02,  8.52477607e-02,\n",
       "         5.42720503e-01, -2.59636211e-01],\n",
       "       [-3.17610211e-01, -1.50355219e-01, -8.85802379e-02,\n",
       "        -3.73058400e-01, -3.90053899e-01,  2.98968958e-02,\n",
       "         3.51303172e-02,  5.80417642e-01,  1.76816927e-01,\n",
       "         4.49497673e-01,  5.70027903e-01, -1.44007392e-01,\n",
       "         1.16319305e-01, -4.75313403e-02, -4.71255955e-03,\n",
       "        -2.90689398e-01,  3.54481132e-01, -1.30702121e-01,\n",
       "         3.26569116e-01,  2.32738580e-01],\n",
       "       [-4.78769183e-02, -1.12226779e-01,  1.41683173e-02,\n",
       "        -1.52111383e-02, -2.51713277e-02, -9.34062957e-03,\n",
       "        -5.28385248e-02,  1.76816927e-01,  4.77682793e-01,\n",
       "         3.26255229e-02,  1.43078816e-01, -1.91608833e-01,\n",
       "         3.74220925e-02,  2.41104965e-02,  4.55665579e-02,\n",
       "        -3.73834516e-01,  2.49021867e-01, -2.32201106e-01,\n",
       "         1.63754713e-03,  2.96905402e-02],\n",
       "       [-1.02233116e-01, -1.86464235e-01, -3.04206515e-01,\n",
       "        -3.37971274e-01, -3.18397214e-01,  6.33343035e-02,\n",
       "         3.08701116e-01,  4.49497673e-01,  3.26255229e-02,\n",
       "         4.73214680e-01,  4.07235286e-01, -9.77416976e-02,\n",
       "        -4.71262267e-02, -5.33397098e-02,  1.04415730e-01,\n",
       "        -1.55295396e-01,  2.22745330e-01, -2.35325414e-02,\n",
       "         4.63959037e-01,  9.70285384e-02],\n",
       "       [-4.27505846e-01, -1.78610892e-02,  3.68463466e-02,\n",
       "        -3.75780499e-01, -4.07562341e-01,  1.77521153e-02,\n",
       "        -8.39132395e-02,  5.70027903e-01,  1.43078816e-01,\n",
       "         4.07235286e-01,  6.07886486e-01, -2.09654893e-01,\n",
       "         1.75892424e-01, -1.58921406e-01, -1.43285654e-01,\n",
       "        -3.07816602e-01,  4.14554809e-01, -2.95602442e-02,\n",
       "         2.38337029e-01,  3.61044166e-01],\n",
       "       [ 1.54976221e-01, -2.13823067e-01, -2.22111984e-02,\n",
       "         9.57344814e-02,  9.81085341e-02, -1.79191714e-02,\n",
       "        -3.73348365e-02, -1.44007392e-01, -1.91608833e-01,\n",
       "        -9.77416976e-02, -2.09654893e-01,  4.86256451e-01,\n",
       "         5.47280009e-03,  4.48495760e-01,  2.31065461e-01,\n",
       "         3.68452920e-01, -4.12062331e-01, -3.20497326e-01,\n",
       "        -6.59038743e-02, -3.23459180e-01],\n",
       "       [-2.88827610e-01,  9.85981776e-02,  3.02193743e-01,\n",
       "        -4.39892610e-02, -9.20077572e-02, -4.35993293e-02,\n",
       "        -3.67686478e-01,  1.16319305e-01,  3.74220925e-02,\n",
       "        -4.71262267e-02,  1.75892424e-01,  5.47280009e-03,\n",
       "         2.16962945e-01, -1.46954018e-03, -1.76517524e-01,\n",
       "        -5.51400839e-02,  9.45095407e-02, -6.89326939e-02,\n",
       "        -1.96569754e-01,  1.84677353e-01],\n",
       "       [ 1.89903947e-01, -3.54340588e-01, -8.38757854e-02,\n",
       "         8.49578415e-02,  8.95718924e-02, -1.69535028e-02,\n",
       "        -5.14407603e-03, -4.75313403e-02,  2.41104965e-02,\n",
       "        -5.33397098e-02, -1.58921406e-01,  4.48495760e-01,\n",
       "        -1.46954018e-03,  5.35528330e-01,  3.40722635e-01,\n",
       "         2.21017377e-01, -3.42976017e-01, -4.85919896e-01,\n",
       "        -1.47835965e-02, -3.91313149e-01],\n",
       "       [ 3.33870699e-01, -3.77933731e-01, -3.60630699e-01,\n",
       "         2.15091577e-02,  6.53183471e-02,  3.42205183e-02,\n",
       "         3.43969692e-01, -4.71255955e-03,  4.55665579e-02,\n",
       "         1.04415730e-01, -1.43285654e-01,  2.31065461e-01,\n",
       "        -1.76517524e-01,  3.40722635e-01,  4.05233542e-01,\n",
       "         1.04261061e-01, -2.21130782e-01, -2.84167297e-01,\n",
       "         2.43244078e-01, -3.91580710e-01],\n",
       "       [ 1.86772591e-01, -2.41244454e-02, -2.01829005e-02,\n",
       "         1.35513611e-01,  1.48567189e-01, -7.34253379e-03,\n",
       "         2.60122862e-02, -2.90689398e-01, -3.73834516e-01,\n",
       "        -1.55295396e-01, -3.07816602e-01,  3.68452920e-01,\n",
       "        -5.51400839e-02,  2.21017377e-01,  1.04261061e-01,\n",
       "         4.36767772e-01, -4.11996918e-01, -1.03933908e-02,\n",
       "        -8.80516753e-02, -2.36787378e-01],\n",
       "       [-3.08547512e-01,  1.28263651e-01,  6.52658956e-02,\n",
       "        -2.17246533e-01, -2.37976824e-01,  1.26867780e-02,\n",
       "        -5.66293668e-02,  3.54481132e-01,  2.49021867e-01,\n",
       "         2.22745330e-01,  4.14554809e-01, -4.12062331e-01,\n",
       "         9.45095407e-02, -3.42976017e-01, -2.21130782e-01,\n",
       "        -4.11996918e-01,  4.71249458e-01,  1.46542438e-01,\n",
       "         1.14656448e-01,  3.77222940e-01],\n",
       "       [-4.90171689e-02,  3.58881489e-01,  2.64017607e-02,\n",
       "        -5.20024225e-03,  5.63472913e-03,  2.16304553e-02,\n",
       "         8.52477607e-02, -1.30702121e-01, -2.32201106e-01,\n",
       "        -2.35325414e-02, -2.95602442e-02, -3.20497326e-01,\n",
       "        -6.89326939e-02, -4.85919896e-01, -2.84167297e-01,\n",
       "        -1.03933908e-02,  1.46542438e-01,  5.62598201e-01,\n",
       "        -5.00266811e-04,  2.66607523e-01],\n",
       "       [ 1.23214863e-01, -2.63960405e-01, -4.98425541e-01,\n",
       "        -2.73346481e-01, -2.21137180e-01,  8.77638499e-02,\n",
       "         5.42720503e-01,  3.26569116e-01,  1.63754713e-03,\n",
       "         4.63959037e-01,  2.38337029e-01, -6.59038743e-02,\n",
       "        -1.96569754e-01, -1.47835965e-02,  2.43244078e-01,\n",
       "        -8.80516753e-02,  1.14656448e-01, -5.00266811e-04,\n",
       "         5.63697267e-01, -6.75998842e-02],\n",
       "       [-4.23509637e-01,  3.13495417e-01,  2.70487406e-01,\n",
       "        -1.74735091e-01, -2.16957566e-01, -1.25871882e-02,\n",
       "        -2.59636211e-01,  2.32738580e-01,  2.96905402e-02,\n",
       "         9.70285384e-02,  3.61044166e-01, -3.23459180e-01,\n",
       "         1.84677353e-01, -3.91313149e-01, -3.91580710e-01,\n",
       "        -2.36787378e-01,  3.77222940e-01,  2.66607523e-01,\n",
       "        -6.75998842e-02,  4.76744046e-01]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons = 20\n",
    "R=10\n",
    "collocation = 10\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-R,R,neurons)\n",
    "biases = np.random.uniform(-R,R,neurons)\n",
    "sigma = sin_function\n",
    "M = mass_matrix(double_neural_net,sigma, weights, biases, neurons)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1985259371182663e-52"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_m = np.linalg.det(M)\n",
    "det_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7010446547906883e+17"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_m = np.linalg.cond(M)\n",
    "cond_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b_matrix(b_neural_net,sigma,weights, biases,f,neurons)\n",
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linalg.solve(M,b)\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11828/1981966750.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n"
     ]
    }
   ],
   "source": [
    "u = u_nn(x,weights,biases,sigma,collocation,neurons,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.4643962 , 2.53972107, 1.91512868, 2.34547873, 2.99187966,\n",
       "       2.43252066, 1.90459509, 2.46927764, 2.50180138, 1.55217831])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Loss: 0.0016409959781178282\n"
     ]
    }
   ],
   "source": [
    "loss = l2_loss(f(x), u)\n",
    "print(\"L2 Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6818c013192d840514133cc15ea0bc15bb95d666290febd40d542c4e37363947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
