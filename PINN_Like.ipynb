{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import inspect\n",
    "import os\n",
    "np.random.seed(0)\n",
    "\n",
    "#Activation functions\n",
    "\n",
    "def sin_function(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def sin_function_derivative(x):\n",
    "    return -np.sin(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return -2 * np.tanh(x) * (1 - np.tanh(x)**2)\n",
    "\n",
    "#Function to approximate\n",
    "def f(x):\n",
    "    return np.sin(3*np.pi*x + 3*np.pi/20) * np.cos(2*np.pi*x + np.pi/10) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the mass matrix - M\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, n):\n",
    "    M = np.zeros([n, n])\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i+1,j+1,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single basis function - Neural Network\n",
    "def single_neural_net(x,weights,biases,i, sigma): \n",
    "\n",
    "    return sigma(weights[i]*x+biases[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basis function multiplication helper for quadrature\n",
    "def double_neural_net(x,weights,biases,i,j, sigma): \n",
    "\n",
    "    return single_neural_net(x,weights,biases,i,sigma)*single_neural_net(x,weights,biases,j,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias helper function for quadrature\n",
    "def b_neural_net(x, weights, biases,i, sigma,f):\n",
    "    return single_neural_net(x,weights,biases,i, sigma)*f(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quadrature for the bias matrix - B\n",
    "def b_matrix(func,sigma, weights, biases,f, neurons):\n",
    "    b = np.zeros([neurons, 1])\n",
    "    for i in range(0, neurons):\n",
    "            b[i] = quad(func, 0, 1, args=(weights, biases,i,sigma,f), limit=1000)[0]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the approximation vector\n",
    "def u_nn(x, weights, biases, sigma,collocation, neurons, a):\n",
    "    u = np.zeros(collocation)\n",
    "    \n",
    "    for j in range(0,collocation):\n",
    "        for i in range(0,neurons):\n",
    "            \n",
    "            u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n",
    "        \n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for eigenvalues and eigenvectors\n",
    "from scipy.linalg import eig\n",
    "def get_eig(M):\n",
    "    D,V = eig(M, left=False, right=True)\n",
    "\n",
    "\n",
    "    idx = D.argsort()[::-1]   \n",
    "    D = D[idx]\n",
    "    V = V[:,idx]\n",
    "\n",
    "    D = np.flip(D)\n",
    "    V = np.fliplr(V)\n",
    "\n",
    "    return D,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate L2 loss\n",
    "def l2_loss(f_x, u_nn_result):\n",
    "    return np.linalg.norm(f_x - u_nn_result)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the Phi matrix, made up of n basis functions\n",
    "def phi_matrix(x,weights,biases,sigma,neurons,collocation):\n",
    "    phi = np.zeros((collocation,neurons))\n",
    "    for i in range(0,neurons):\n",
    "        for j in range(0,collocation):\n",
    "\n",
    "            phi[j,i] = sigma(weights[i]*x[j]+biases[i])\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=False):\n",
    "    phi = phi_matrix(x,weights,biases,sigma,neurons,collocation)\n",
    "    f_func = f(x)\n",
    "    a = np.linalg.pinv(phi)@f_func\n",
    "\n",
    "    u = u_nn(x, weights, biases, sigma, collocation,neurons, a)\n",
    "\n",
    "    loss = l2_loss(f_func, u)\n",
    "    if verbose:\n",
    "        print(\"=\"*20)\n",
    "        print(\"Sigma:\", sigma.__name__)\n",
    "        print(\"Number of collocation points:\", collocation)\n",
    "        print(\"Number of neurons:\", neurons)\n",
    "        print(\"L2 Loss:\", loss)\n",
    "        print(\"=\"*20)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_list(data, list_name, sheet_name, directory=None, columns=None, overwrite=False):\n",
    "    default_directory = '/mnt/c/Git_Repos/Function_approximation/Results'\n",
    "    file_name = f\"{list_name}.xlsx\"\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = [\"Loss\", \"# of Collocation\", \"# of Neurons\", \"Activation Function\", \"R - Sample Range\"]\n",
    "    \n",
    "    if directory is None:\n",
    "        directory = default_directory\n",
    "    \n",
    "    full_path = os.path.join(directory, file_name)\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    if os.path.exists(full_path):\n",
    "        try:\n",
    "            # If the file exists\n",
    "            with pd.ExcelWriter(full_path, engine='openpyxl', mode='a') as writer:\n",
    "                if overwrite:\n",
    "                    # If overwrite is True, delete the sheet if it exists\n",
    "                    if sheet_name in writer.book.sheetnames:\n",
    "                        writer.book.remove(writer.book[sheet_name])\n",
    "                # Write the DataFrame to the Excel file\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except ValueError as e:\n",
    "            # If the sheet already exists and overwrite is False, print a message and continue\n",
    "            print(f\"Sheet name '{sheet_name}' already exists in the file '{file_name}'\")\n",
    "    else:\n",
    "        # If the file does not exist, create a new one\n",
    "        df.to_excel(full_path, index=False, sheet_name=sheet_name)\n",
    "\n",
    "    print(f\"Data exported to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_values(neurons, collocations, sigmas, Rs, sheet_name, xmin=0, xmax=1, overwrite=False, verbose=False):\n",
    "    loss_values = []\n",
    "    #np.random.seed(0)  # Set a seed for reproducibility\n",
    "    for sigma in sigmas:\n",
    "        for neuron in neurons:\n",
    "            for collocation in collocations:\n",
    "                for R in Rs:\n",
    "                    x = np.linspace(xmin, xmax, collocation)\n",
    "                    weights = np.random.uniform(-R, R, neuron)\n",
    "                    biases = np.random.uniform(-R, R, neuron)\n",
    "                    loss = evaluate_func_approx(x, f, weights, biases, sigma, neuron, collocation, verbose=verbose)\n",
    "                    loss_values.append((loss, collocation, neuron, sigma.__name__,R))  # Add activation function name\n",
    "    export_list(loss_values, \"Finite_Neuron_Method_Results\", sheet_name, overwrite=overwrite)\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_collocation(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by collocation point, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each collocation point\n",
    "    collocation_loss_data = {collocation: [] for _, collocation, _, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (neuron, loss, activation_func_name, R) tuples for each collocation point\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        collocation_loss_data[collocation].append((neuron, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the collocation points and print the relevant data\n",
    "        for collocation, data in collocation_loss_data.items():\n",
    "            print(f\"# of Collocation points: {collocation}\")\n",
    "            \n",
    "            # Print the neurons, loss values, and R for the current activation function\n",
    "            for neuron, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Neurons: {neuron}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_neuron(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by neuron, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each neuron\n",
    "    neuron_loss_data = {neuron: [] for _, _, neuron, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, loss, activation_func_name, R) tuples for each neuron\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        neuron_loss_data[neuron].append((collocation, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the neurons and print the relevant data\n",
    "        for neuron, data in neuron_loss_data.items():\n",
    "            print(f\"# of Neurons: {neuron}\")\n",
    "            \n",
    "            # Print the collocation points, loss values, and R for the current activation function\n",
    "            for collocation, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_R(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by the range R, activation function, collocation points, and neurons.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each R value\n",
    "    R_loss_data = {R: [] for _, _, _, _, R in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, neuron, loss, activation_func_name) tuples for each R value\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        R_loss_data[R].append((collocation, neuron, loss, activation_func_name))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the R values and print the relevant data\n",
    "        for R, data in R_loss_data.items():\n",
    "            print(f\"Range R: {R}\")\n",
    "            \n",
    "            # Print the collocation points, neurons, and loss values for the current activation function\n",
    "            for collocation, neuron, loss, act_func_name in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, # of Neurons: {neuron}, Loss: {loss:.10e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Sigma: sin_function2_derivative\n",
      "Number of collocation points: 50\n",
      "Number of neurons: 25\n",
      "L2 Loss: 1.8921620734168687e-05\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "collocation = 50\n",
    "neurons = 25\n",
    "sigma = sin_function2_derivative\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-5,5,neurons)\n",
    "biases = np.random.uniform(-5,5,neurons)\n",
    "test = evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Increasing #of neurons\n",
    "Keeping collocations fixed at 50, and using ReLU, tanh, and sin as activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Increasing # of Neurons' already exists in the file 'Finite_Neuron_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n",
      "Activation Function: relu_derivative\n",
      "Range R: 1\n",
      "  # of Collocation points: 50, # of Neurons: 5, Loss: 5.7329351727e+00\n",
      "  # of Collocation points: 50, # of Neurons: 10, Loss: 4.2788469935e+00\n",
      "  # of Collocation points: 50, # of Neurons: 20, Loss: 1.7828646175e+00\n",
      "  # of Collocation points: 50, # of Neurons: 30, Loss: 2.1174095914e+00\n",
      "  # of Collocation points: 50, # of Neurons: 40, Loss: 2.5218548293e+00\n",
      "  # of Collocation points: 50, # of Neurons: 50, Loss: 8.0051356822e-01\n",
      "  # of Collocation points: 50, # of Neurons: 100, Loss: 4.0705972680e-01\n",
      "Activation Function: sin_function_derivative\n",
      "Range R: 1\n",
      "  # of Collocation points: 50, # of Neurons: 5, Loss: 5.3105737543e+00\n",
      "  # of Collocation points: 50, # of Neurons: 10, Loss: 2.3139909258e-01\n",
      "  # of Collocation points: 50, # of Neurons: 20, Loss: 4.5283380323e-02\n",
      "  # of Collocation points: 50, # of Neurons: 30, Loss: 4.5456123569e-02\n",
      "  # of Collocation points: 50, # of Neurons: 40, Loss: 4.4921953748e-02\n",
      "  # of Collocation points: 50, # of Neurons: 50, Loss: 4.5382756446e-02\n",
      "  # of Collocation points: 50, # of Neurons: 100, Loss: 4.5569319597e-02\n",
      "Activation Function: tanh_derivative\n",
      "Range R: 1\n",
      "  # of Collocation points: 50, # of Neurons: 5, Loss: 5.0658659870e+00\n",
      "  # of Collocation points: 50, # of Neurons: 10, Loss: 5.5875176855e-02\n",
      "  # of Collocation points: 50, # of Neurons: 20, Loss: 4.8715698154e-05\n",
      "  # of Collocation points: 50, # of Neurons: 30, Loss: 8.4711564713e-04\n",
      "  # of Collocation points: 50, # of Neurons: 40, Loss: 8.6277461319e-05\n",
      "  # of Collocation points: 50, # of Neurons: 50, Loss: 1.8781152238e-04\n",
      "  # of Collocation points: 50, # of Neurons: 100, Loss: 1.9110740136e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "neurons = [5,10,20,30,40,50,100]\n",
    "sigmas = [relu_derivative,tanh_derivative,sin_function_derivative]\n",
    "collocations = [50]\n",
    "Rs = [1]\n",
    "loss_values_increasing_neurons = compute_loss_values(neurons, collocations, sigmas, Rs, \"Increasing # of Neurons\", overwrite=False, verbose=False)\n",
    "print_loss_by_R(loss_values_increasing_neurons)\n",
    "#loss_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - Increasing Collocation points\n",
    "Keeping neurons fixed at 20, and the same activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Increasing # of Collocation Points' already exists in the file 'Finite_Neuron_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n",
      "Activation Function: relu_derivative\n",
      "# of Neurons: 20\n",
      "  # of Collocation points: 10, Loss: 1.2553876083e+00, R: 1\n",
      "  # of Collocation points: 50, Loss: 4.5440930105e+00, R: 1\n",
      "  # of Collocation points: 100, Loss: 1.1309512322e+01, R: 1\n",
      "  # of Collocation points: 200, Loss: 2.4029897539e+01, R: 1\n",
      "  # of Collocation points: 500, Loss: 2.9035023417e+01, R: 1\n",
      "Activation Function: sin_function_derivative\n",
      "# of Neurons: 20\n",
      "  # of Collocation points: 10, Loss: 1.1996700803e-04, R: 1\n",
      "  # of Collocation points: 50, Loss: 4.4664400298e-02, R: 1\n",
      "  # of Collocation points: 100, Loss: 9.3847188837e-02, R: 1\n",
      "  # of Collocation points: 200, Loss: 1.6576035139e-01, R: 1\n",
      "  # of Collocation points: 500, Loss: 3.8670733803e-01, R: 1\n",
      "Activation Function: tanh_derivative\n",
      "# of Neurons: 20\n",
      "  # of Collocation points: 10, Loss: 1.9503066950e-15, R: 1\n",
      "  # of Collocation points: 50, Loss: 1.1698717248e-03, R: 1\n",
      "  # of Collocation points: 100, Loss: 4.9049267572e-06, R: 1\n",
      "  # of Collocation points: 200, Loss: 3.8582090662e-05, R: 1\n",
      "  # of Collocation points: 500, Loss: 6.9550740092e-04, R: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "neurons = [20]\n",
    "sigmas = [relu_derivative,tanh_derivative,sin_function_derivative]\n",
    "collocations = [10,50,100,200,500]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Increasing # of Collocation Points\",overwrite=False,verbose=False)\n",
    "print_loss_by_neuron(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(loss_values, columns=['Loss', 'Collocation', 'Neurons', 'Activation Function', 'R'])\n",
    "\n",
    "# Sort DataFrame by 'Loss' column\n",
    "sorted_df = df.sort_values(by='Loss')\n",
    "\n",
    "# print(\"Sorted by Loss\")\n",
    "# print(sorted_df)\n",
    "# print(\"=\"*80)\n",
    "# print(\"\")\n",
    "# print(\"Filtered for Sin Activation Function\")\n",
    "# filtered_df = df[df[\"Activation Function\"] == \"sin_function\"]\n",
    "\n",
    "# print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_neuron(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 - Increasing the sampling range R\n",
    "Keeping neurons and collocations fixed, increasing R to increase chance that w_i and b_i are further away from w_j and b_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Increasing R' already exists in the file 'Finite_Neuron_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "neurons = [30]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "sigmas_derivative = [tanh_derivative,relu_derivative,sin_function_derivative]\n",
    "collocations = [100]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Increasing R\",overwrite=False,verbose=False)\n",
    "derivative_loss_values = compute_loss_values(neurons, collocations,sigmas_derivative,Rs,\"Increasing R\",overwrite=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0003455134170967644, 100, 30, 'tanh_derivative', 1),\n",
       " (5.432211212234588e-05, 100, 30, 'tanh_derivative', 2),\n",
       " (3.5348750496849822e-06, 100, 30, 'tanh_derivative', 4),\n",
       " (3.811423845927587e-05, 100, 30, 'tanh_derivative', 5),\n",
       " (6.170415123650935e-10, 100, 30, 'tanh_derivative', 10),\n",
       " (5.7168269891407615, 100, 30, 'relu_derivative', 1),\n",
       " (5.609330132244446, 100, 30, 'relu_derivative', 2),\n",
       " (6.575301635364382, 100, 30, 'relu_derivative', 4),\n",
       " (5.416792150516557, 100, 30, 'relu_derivative', 5),\n",
       " (6.335727090258714, 100, 30, 'relu_derivative', 10),\n",
       " (0.08422150807032926, 100, 30, 'sin_function_derivative', 1),\n",
       " (0.0017428802773873093, 100, 30, 'sin_function_derivative', 2),\n",
       " (1.9056814726028912e-05, 100, 30, 'sin_function_derivative', 4),\n",
       " (2.113425450170623e-05, 100, 30, 'sin_function_derivative', 5),\n",
       " (7.846881351737049e-05, 100, 30, 'sin_function_derivative', 10)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.309896953251292e-05, 100, 30, 'tanh', 1),\n",
       " (0.00048619058656185893, 100, 30, 'tanh', 2),\n",
       " (2.1674058882000564e-07, 100, 30, 'tanh', 4),\n",
       " (1.7280728325284913e-05, 100, 30, 'tanh', 5),\n",
       " (0.00014075296618092363, 100, 30, 'tanh', 10),\n",
       " (3.307905316179587, 100, 30, 'relu', 1),\n",
       " (6.814174842179699, 100, 30, 'relu', 2),\n",
       " (2.0275428797723904, 100, 30, 'relu', 4),\n",
       " (9.25612921059808, 100, 30, 'relu', 5),\n",
       " (8.911102213415985, 100, 30, 'relu', 10),\n",
       " (0.08528242352099909, 100, 30, 'sin_function', 1),\n",
       " (0.0017284368406900539, 100, 30, 'sin_function', 2),\n",
       " (1.602402138023601e-05, 100, 30, 'sin_function', 4),\n",
       " (3.992694431729737e-05, 100, 30, 'sin_function', 5),\n",
       " (0.0036673028336331143, 100, 30, 'sin_function', 10)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to combined_loss_values.xlsx\n"
     ]
    }
   ],
   "source": [
    "comparison_values = loss_values + derivative_loss_values\n",
    "df = pd.DataFrame(comparison_values, columns=[\"Loss\", \"# of Collocation\", \"# of Neurons\", \"Activation Function\", \"R - Sample Range\"])\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel(\"combined_loss_values.xlsx\", index=False)\n",
    "\n",
    "print(\"Data exported to combined_loss_values.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(derivative_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss_values(loss_values, derivative_loss_values):\n",
    "    function_dict = {(func, r): err for err, _, _, func, r in loss_values}\n",
    "    derivative_dict = {(func, r): err for err, _, _, func, r in derivative_loss_values}\n",
    "\n",
    "    def base_function_name(derivative_name):\n",
    "        if derivative_name.endswith('_derivative'):\n",
    "            return derivative_name.replace('_derivative', '')\n",
    "        return None\n",
    "\n",
    "    comparison_results = []\n",
    "    for (func_name, r), err in derivative_dict.items():\n",
    "        base_name = base_function_name(func_name)\n",
    "        if base_name:\n",
    "            base_err = function_dict.get((base_name, r))\n",
    "            if base_err is not None:\n",
    "                comparison_results.append((base_name, r, base_err, err))\n",
    "\n",
    "    print(f\"{'Function':<20}{'R Value':<10}{'Function Error':<20}{'Derivative Error':<25}\")\n",
    "    print(\"=\"*80)\n",
    "    for base_name, r, base_err, der_err in comparison_results:\n",
    "        print(f\"{base_name:<20}{r:<10}{base_err:<20.10e}{der_err:<25.10e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            R Value   Function Error      Derivative Error         \n",
      "================================================================================\n",
      "tanh                1         3.3098969533e-05    3.4551341710e-04         \n",
      "tanh                2         4.8619058656e-04    5.4322112122e-05         \n",
      "tanh                4         2.1674058882e-07    3.5348750497e-06         \n",
      "tanh                5         1.7280728325e-05    3.8114238459e-05         \n",
      "tanh                10        1.4075296618e-04    6.1704151237e-10         \n",
      "relu                1         3.3079053162e+00    5.7168269891e+00         \n",
      "relu                2         6.8141748422e+00    5.6093301322e+00         \n",
      "relu                4         2.0275428798e+00    6.5753016354e+00         \n",
      "relu                5         9.2561292106e+00    5.4167921505e+00         \n",
      "relu                10        8.9111022134e+00    6.3357270903e+00         \n",
      "sin_function        1         8.5282423521e-02    8.4221508070e-02         \n",
      "sin_function        2         1.7284368407e-03    1.7428802774e-03         \n",
      "sin_function        4         1.6024021380e-05    1.9056814726e-05         \n",
      "sin_function        5         3.9926944317e-05    2.1134254502e-05         \n",
      "sin_function        10        3.6673028336e-03    7.8468813517e-05         \n"
     ]
    }
   ],
   "source": [
    "compare_loss_values(loss_values,derivative_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "neurons = [10,20,30,50,100]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [5,10,30,50,100]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Full_tests\",overwrite=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, neurons):\n",
    "    M = np.zeros([neurons, neurons])\n",
    "    \n",
    "    for i in range(0, neurons):\n",
    "        for j in range(0, neurons):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i,j,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 100\n",
    "R = 10\n",
    "collocation = 1000\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-R,R,neurons)\n",
    "biases = np.random.uniform(-R,R,neurons)\n",
    "sigma = tanh_derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_system(x, neurons, collocation, weights, biases, sigma, f):\n",
    "\n",
    "    M = mass_matrix(double_neural_net,sigma, weights, biases, neurons)\n",
    "    b = b_matrix(b_neural_net,sigma,weights, biases,f,neurons)\n",
    "    a = np.linalg.solve(M,b)\n",
    "    u = u_nn(x,weights,biases,sigma,collocation,neurons,a)\n",
    "    \n",
    "    loss = l2_loss(f(x), u)\n",
    "    print(\"L2 Loss:\", loss)\n",
    "\n",
    "    return M, b, a, u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23439/1981966750.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Loss: 3.0405575746047427e-05\n"
     ]
    }
   ],
   "source": [
    "M, b, a, u = create_linear_system(x, neurons, collocation, weights, biases, sigma, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6818c013192d840514133cc15ea0bc15bb95d666290febd40d542c4e37363947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
