{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import inspect\n",
    "import os\n",
    "np.random.seed(0)\n",
    "\n",
    "#Activation functions\n",
    "\n",
    "def sin_function(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def sin_function_derivative(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "#Function to approximate\n",
    "def f(x):\n",
    "    return np.sin(3*np.pi*x + 3*np.pi/20) * np.cos(2*np.pi*x + np.pi/10) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the mass matrix - M\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, n):\n",
    "    M = np.zeros([n, n])\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i+1,j+1,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single basis function - Neural Network\n",
    "def single_neural_net(x,weights,biases,i, sigma): \n",
    "\n",
    "    return sigma(weights[i]*x+biases[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basis function multiplication helper for quadrature\n",
    "def double_neural_net(x,weights,biases,i,j, sigma): \n",
    "\n",
    "    return single_neural_net(x,weights,biases,i,sigma)*single_neural_net(x,weights,biases,j,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias helper function for quadrature\n",
    "def b_neural_net(x, weights, biases,i, sigma,f):\n",
    "    return single_neural_net(x,weights,biases,i, sigma)*f(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quadrature for the bias matrix - B\n",
    "def b_matrix(func,sigma, weights, biases,f, neurons):\n",
    "    b = np.zeros([neurons, 1])\n",
    "    for i in range(0, neurons):\n",
    "            b[i] = quad(func, 0, 1, args=(weights, biases,i,sigma,f), limit=1000)[0]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the approximation vector\n",
    "def u_nn(x, weights, biases, sigma,collocation, neurons, a):\n",
    "    u = np.zeros(collocation)\n",
    "    \n",
    "    for j in range(0,collocation):\n",
    "        for i in range(0,neurons):\n",
    "            \n",
    "            u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n",
    "        \n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for eigenvalues and eigenvectors\n",
    "from scipy.linalg import eig\n",
    "def get_eig(M):\n",
    "    D,V = eig(M, left=False, right=True)\n",
    "\n",
    "\n",
    "    idx = D.argsort()[::-1]   \n",
    "    D = D[idx]\n",
    "    V = V[:,idx]\n",
    "\n",
    "    D = np.flip(D)\n",
    "    V = np.fliplr(V)\n",
    "\n",
    "    return D,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate L2 loss\n",
    "def l2_loss(f_x, u_nn_result):\n",
    "    return np.linalg.norm(f_x - u_nn_result)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the Phi matrix, made up of n basis functions\n",
    "def phi_matrix(x,weights,biases,sigma,neurons,collocation):\n",
    "    phi = np.zeros((collocation,neurons))\n",
    "    for i in range(0,neurons):\n",
    "        for j in range(0,collocation):\n",
    "\n",
    "            phi[j,i] = sigma(weights[i]*x[j]+biases[i])\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=False):\n",
    "    phi = phi_matrix(x,weights,biases,sigma,neurons,collocation)\n",
    "    f_func = f(x)\n",
    "    a = np.linalg.pinv(phi)@f_func\n",
    "\n",
    "    u = u_nn(x, weights, biases, sigma, collocation,neurons, a)\n",
    "\n",
    "    loss = l2_loss(f_func, u)\n",
    "    if verbose:\n",
    "        print(\"=\"*20)\n",
    "        print(\"Sigma:\", sigma.__name__)\n",
    "        print(\"Number of collocation points:\", collocation)\n",
    "        print(\"Number of neurons:\", neurons)\n",
    "        print(\"L2 Loss:\", loss)\n",
    "        print(\"=\"*20)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_list(data, list_name, sheet_name, directory=None, columns=None, overwrite=False):\n",
    "    default_directory = '/mnt/c/Git_Repos/Function_approximation/Results'\n",
    "    file_name = f\"{list_name}.xlsx\"\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = [\"Loss\", \"# of Collocation\", \"# of Neurons\", \"Activation Function\", \"R - Sample Range\"]\n",
    "    \n",
    "    if directory is None:\n",
    "        directory = default_directory\n",
    "    \n",
    "    full_path = os.path.join(directory, file_name)\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    if os.path.exists(full_path):\n",
    "        try:\n",
    "            # If the file exists\n",
    "            with pd.ExcelWriter(full_path, engine='openpyxl', mode='a') as writer:\n",
    "                if overwrite:\n",
    "                    # If overwrite is True, delete the sheet if it exists\n",
    "                    if sheet_name in writer.book.sheetnames:\n",
    "                        writer.book.remove(writer.book[sheet_name])\n",
    "                # Write the DataFrame to the Excel file\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except ValueError as e:\n",
    "            # If the sheet already exists and overwrite is False, print a message and continue\n",
    "            print(f\"Sheet name '{sheet_name}' already exists in the file '{file_name}'\")\n",
    "    else:\n",
    "        # If the file does not exist, create a new one\n",
    "        df.to_excel(full_path, index=False, sheet_name=sheet_name)\n",
    "\n",
    "    print(f\"Data exported to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_values(neurons, collocations, sigmas, Rs, sheet_name, xmin=0, xmax=1, overwrite=False, verbose=False):\n",
    "    loss_values = []\n",
    "    #np.random.seed(0)  # Set a seed for reproducibility\n",
    "    for sigma in sigmas:\n",
    "        for neuron in neurons:\n",
    "            for collocation in collocations:\n",
    "                for R in Rs:\n",
    "                    x = np.linspace(xmin, xmax, collocation)\n",
    "                    weights = np.random.uniform(-R, R, neuron)\n",
    "                    biases = np.random.uniform(-R, R, neuron)\n",
    "                    loss = evaluate_func_approx(x, f, weights, biases, sigma, neuron, collocation, verbose=verbose)\n",
    "                    loss_values.append((loss, collocation, neuron, sigma.__name__,R))  # Add activation function name\n",
    "    export_list(loss_values, \"Finite_Neuron_Method_Results\", sheet_name, overwrite=overwrite)\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_collocation(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by collocation point, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each collocation point\n",
    "    collocation_loss_data = {collocation: [] for _, collocation, _, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (neuron, loss, activation_func_name, R) tuples for each collocation point\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        collocation_loss_data[collocation].append((neuron, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the collocation points and print the relevant data\n",
    "        for collocation, data in collocation_loss_data.items():\n",
    "            print(f\"# of Collocation points: {collocation}\")\n",
    "            \n",
    "            # Print the neurons, loss values, and R for the current activation function\n",
    "            for neuron, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Neurons: {neuron}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_neuron(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by neuron, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each neuron\n",
    "    neuron_loss_data = {neuron: [] for _, _, neuron, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, loss, activation_func_name, R) tuples for each neuron\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        neuron_loss_data[neuron].append((collocation, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the neurons and print the relevant data\n",
    "        for neuron, data in neuron_loss_data.items():\n",
    "            print(f\"# of Neurons: {neuron}\")\n",
    "            \n",
    "            # Print the collocation points, loss values, and R for the current activation function\n",
    "            for collocation, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_R(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by the range R, activation function, collocation points, and neurons.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each R value\n",
    "    R_loss_data = {R: [] for _, _, _, _, R in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, neuron, loss, activation_func_name) tuples for each R value\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        R_loss_data[R].append((collocation, neuron, loss, activation_func_name))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the R values and print the relevant data\n",
    "        for R, data in R_loss_data.items():\n",
    "            print(f\"Range R: {R}\")\n",
    "            \n",
    "            # Print the collocation points, neurons, and loss values for the current activation function\n",
    "            for collocation, neuron, loss, act_func_name in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, # of Neurons: {neuron}, Loss: {loss:.10e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Sigma: sin_function\n",
      "Number of collocation points: 100\n",
      "Number of neurons: 50\n",
      "L2 Loss: 0.08512083411934636\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "collocation = 100\n",
    "neurons = 50\n",
    "sigma = sin_function\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-1,1,neurons)\n",
    "biases = np.random.uniform(-1,1,neurons)\n",
    "test = evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Increasing #of neurons\n",
    "Keeping collocations fixed at 50, and using ReLU, tanh, and sin as activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "neurons = [5,10,20,30,40,50,100]\n",
    "sigmas = [relu_derivative,tanh_derivative,sin_function_derivative]\n",
    "collocations = [50]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations, sigmas, Rs, \"Increasing # of Neurons\", overwrite=True, verbose=False)\n",
    "#print_loss_by_R(loss_values_increasing_neurons)\n",
    "#loss_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - Increasing Collocation points\n",
    "Keeping neurons fixed at 20, and the same activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "neurons = [20]\n",
    "sigmas = [relu_derivative,tanh_derivative,sin_function_derivative]\n",
    "collocations = [10,50,100,200,500]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Increasing # of Collocation Points\",overwrite=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Loss\n",
      "        Loss  Collocation  Neurons Activation Function   R\n",
      "13  0.000002  50           30       sin_function        5 \n",
      "1   0.000017  50           30       tanh                2 \n",
      "3   0.000037  50           30       tanh                5 \n",
      "0   0.000128  50           30       tanh                1 \n",
      "14  0.000202  50           30       sin_function        10\n",
      "4   0.000257  50           30       tanh                10\n",
      "12  0.000656  50           30       sin_function        4 \n",
      "11  0.000814  50           30       sin_function        2 \n",
      "2   0.001043  50           30       tanh                4 \n",
      "10  0.045561  50           30       sin_function        1 \n",
      "5   0.858394  50           30       relu                1 \n",
      "8   1.508541  50           30       relu                5 \n",
      "9   2.265392  50           30       relu                10\n",
      "7   2.906647  50           30       relu                4 \n",
      "6   3.156628  50           30       relu                2 \n",
      "================================================================================\n",
      "\n",
      "Filtered for Sin Activation Function\n",
      "        Loss  Collocation  Neurons Activation Function   R\n",
      "10  0.045561  50           30       sin_function        1 \n",
      "11  0.000814  50           30       sin_function        2 \n",
      "12  0.000656  50           30       sin_function        4 \n",
      "13  0.000002  50           30       sin_function        5 \n",
      "14  0.000202  50           30       sin_function        10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(loss_values, columns=['Loss', 'Collocation', 'Neurons', 'Activation Function', 'R'])\n",
    "\n",
    "# Sort DataFrame by 'Loss' column\n",
    "sorted_df = df.sort_values(by='Loss')\n",
    "\n",
    "print(\"Sorted by Loss\")\n",
    "print(sorted_df)\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"Filtered for Sin Activation Function\")\n",
    "filtered_df = df[df[\"Activation Function\"] == \"sin_function\"]\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_neuron(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 - Increasing the sampling range R\n",
    "Keeping neurons and collocations fixed, increasing R to increase chance that w_i and b_i are further away from w_j and b_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Increasing R' already exists in the file 'Finite_Neuron_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n",
      "Sheet name 'Increasing R - Derivative activations' already exists in the file 'Finite_Neuron_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n"
     ]
    }
   ],
   "source": [
    "neurons = [30]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "sigmas_derivative = [tanh_derivative,relu_derivative,sin_function_derivative]\n",
    "collocations = [50]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Increasing R\",verbose=False)\n",
    "derivative_loss_values = compute_loss_values(neurons, collocations,sigmas_derivative,Rs,\"Increasing R - Derivative activations\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to combined_loss_values.xlsx\n"
     ]
    }
   ],
   "source": [
    "comparison_values = loss_values + derivative_loss_values\n",
    "df = pd.DataFrame(comparison_values, columns=[\"Loss\", \"# of Collocation\", \"# of Neurons\", \"Activation Function\", \"R - Sample Range\"])\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel(\"combined_loss_values.xlsx\", index=False)\n",
    "\n",
    "print(\"Data exported to combined_loss_values.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th># of Collocation</th>\n",
       "      <th># of Neurons</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>R - Sample Range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.278497e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.692095e-05</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.043494e-03</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.653896e-05</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.567580e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.583941e-01</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.156628e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.906647e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.508541e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.265392e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.556076e-02</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.137848e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.558036e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.900746e-06</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.018940e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.301239e-06</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh_derivative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.417234e-06</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh_derivative</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.923103e-06</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh_derivative</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.645828e-08</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh_derivative</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.916437e-09</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh_derivative</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.108453e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu_derivative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.061542e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu_derivative</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.681995e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu_derivative</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.574118e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu_derivative</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.951001e+00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>relu_derivative</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.325581e-02</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function_derivative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.582635e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function_derivative</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.045534e-05</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function_derivative</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.044640e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function_derivative</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.427759e-06</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>sin_function_derivative</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Loss  # of Collocation  # of Neurons      Activation Function  \\\n",
       "0   1.278497e-04                50            30                     tanh   \n",
       "1   1.692095e-05                50            30                     tanh   \n",
       "2   1.043494e-03                50            30                     tanh   \n",
       "3   3.653896e-05                50            30                     tanh   \n",
       "4   2.567580e-04                50            30                     tanh   \n",
       "5   8.583941e-01                50            30                     relu   \n",
       "6   3.156628e+00                50            30                     relu   \n",
       "7   2.906647e+00                50            30                     relu   \n",
       "8   1.508541e+00                50            30                     relu   \n",
       "9   2.265392e+00                50            30                     relu   \n",
       "10  4.556076e-02                50            30             sin_function   \n",
       "11  8.137848e-04                50            30             sin_function   \n",
       "12  6.558036e-04                50            30             sin_function   \n",
       "13  1.900746e-06                50            30             sin_function   \n",
       "14  2.018940e-04                50            30             sin_function   \n",
       "15  9.301239e-06                50            30          tanh_derivative   \n",
       "16  2.417234e-06                50            30          tanh_derivative   \n",
       "17  3.923103e-06                50            30          tanh_derivative   \n",
       "18  6.645828e-08                50            30          tanh_derivative   \n",
       "19  4.916437e-09                50            30          tanh_derivative   \n",
       "20  2.108453e+00                50            30          relu_derivative   \n",
       "21  4.061542e+00                50            30          relu_derivative   \n",
       "22  3.681995e+00                50            30          relu_derivative   \n",
       "23  3.574118e+00                50            30          relu_derivative   \n",
       "24  2.951001e+00                50            30          relu_derivative   \n",
       "25  4.325581e-02                50            30  sin_function_derivative   \n",
       "26  9.582635e-04                50            30  sin_function_derivative   \n",
       "27  1.045534e-05                50            30  sin_function_derivative   \n",
       "28  3.044640e-04                50            30  sin_function_derivative   \n",
       "29  4.427759e-06                50            30  sin_function_derivative   \n",
       "\n",
       "    R - Sample Range  \n",
       "0                  1  \n",
       "1                  2  \n",
       "2                  4  \n",
       "3                  5  \n",
       "4                 10  \n",
       "5                  1  \n",
       "6                  2  \n",
       "7                  4  \n",
       "8                  5  \n",
       "9                 10  \n",
       "10                 1  \n",
       "11                 2  \n",
       "12                 4  \n",
       "13                 5  \n",
       "14                10  \n",
       "15                 1  \n",
       "16                 2  \n",
       "17                 4  \n",
       "18                 5  \n",
       "19                10  \n",
       "20                 1  \n",
       "21                 2  \n",
       "22                 4  \n",
       "23                 5  \n",
       "24                10  \n",
       "25                 1  \n",
       "26                 2  \n",
       "27                 4  \n",
       "28                 5  \n",
       "29                10  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(derivative_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            R Value   Function Error      Derivative Error         \n",
      "================================================================================\n",
      "tanh                1         2.3434375050e-05    3.2283148589e-06         \n",
      "tanh                2         3.5857798345e-05    6.3889155757e-04         \n",
      "tanh                4         2.3862848033e-09    1.4912348109e-06         \n",
      "tanh                5         7.0521932236e-04    3.8311405099e-08         \n",
      "tanh                10        1.1318563900e-03    4.0555510274e-10         \n",
      "relu                1         1.1869997774e+00    2.5778510505e+00         \n",
      "relu                2         2.8164332146e+00    3.6305058863e+00         \n",
      "relu                4         4.5088958398e+00    2.7595219648e+00         \n",
      "relu                5         2.9962341889e+00    3.7695511148e+00         \n",
      "relu                10        4.0318911069e-01    1.7786649808e+00         \n",
      "sin_function        1         4.5290035084e-02    4.3091522541e-02         \n",
      "sin_function        2         8.5666752506e-04    8.9595396603e-04         \n",
      "sin_function        4         8.6480585223e-06    1.9468067455e-05         \n",
      "sin_function        5         8.6704699712e-06    4.9940715184e-04         \n",
      "sin_function        10        2.1662102256e-03    1.3129232027e-03         \n"
     ]
    }
   ],
   "source": [
    "function_dict = {(func, r): err for err, _, _, func, r in loss_values}\n",
    "derivative_dict = {(func, r): err for err, _, _, func, r in derivative_loss_values}\n",
    "\n",
    "def base_function_name(derivative_name):\n",
    "    if derivative_name.endswith('_derivative'):\n",
    "        return derivative_name.replace('_derivative', '')\n",
    "    return None\n",
    "\n",
    "comparison_results = []\n",
    "for (func_name, r), err in derivative_dict.items():\n",
    "    base_name = base_function_name(func_name)\n",
    "    if base_name:\n",
    "        base_err = function_dict.get((base_name, r))\n",
    "        if base_err is not None:\n",
    "            comparison_results.append((base_name, r, base_err, err))\n",
    "\n",
    "print(f\"{'Function':<20}{'R Value':<10}{'Function Error':<20}{'Derivative Error':<25}\")\n",
    "print(\"=\"*80)\n",
    "for base_name, r, base_err, der_err in comparison_results:\n",
    "    print(f\"{base_name:<20}{r:<10}{base_err:<20.10e}{der_err:<25.10e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name 'Full_tests' already exists in the file 'Finite_Neuron_Method_Results.xlsx'\n",
      "Data exported to /mnt/c/Git_Repos/Function_approximation/Results/Finite_Neuron_Method_Results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zephunix/miniconda3/envs/tf/lib/python3.10/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "neurons = [10,20,30,50,100]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [5,10,30,50,100]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,\"Full_tests\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_loss_by_collocation(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, neurons):\n",
    "    M = np.zeros([neurons, neurons])\n",
    "    \n",
    "    for i in range(0, neurons):\n",
    "        for j in range(0, neurons):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i,j,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 10\n",
    "R = 10\n",
    "weights = np.random.uniform(-R,R,neurons)\n",
    "biases = np.random.uniform(-R,R,neurons)\n",
    "sigma = tanh\n",
    "M = mass_matrix(double_neural_net,sigma, weights, biases, neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_m = np.linalg.det(M)\n",
    "#det_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b_matrix(b_neural_net,sigma,weights, biases,f,neurons)\n",
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linalg.solve(M,b)\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21123/1981966750.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n"
     ]
    }
   ],
   "source": [
    "collocation = 100\n",
    "x = np.linspace(0,1,collocation)\n",
    "u = u_nn(x,weights,biases,sin_function,collocation,neurons,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Loss: 41210325357564.1\n"
     ]
    }
   ],
   "source": [
    "loss = l2_loss(f(x), u)\n",
    "print(\"L2 Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6818c013192d840514133cc15ea0bc15bb95d666290febd40d542c4e37363947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
