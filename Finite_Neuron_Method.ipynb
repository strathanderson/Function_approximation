{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "#Activation functions\n",
    "\n",
    "def sin_function(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def sin_function_derivative(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "#Function to approximate\n",
    "def f(x):\n",
    "    return np.sin(3*np.pi*x + 3*np.pi/20) * np.cos(2*np.pi*x + np.pi/10) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the mass matrix - M\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, n):\n",
    "    M = np.zeros([n, n])\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i+1,j+1,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single basis function - Neural Network\n",
    "def single_neural_net(x,weights,biases,i, sigma): \n",
    "\n",
    "    return sigma(weights[i]*x+biases[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basis function multiplication helper for quadrature\n",
    "def double_neural_net(x,weights,biases,i,j, sigma): \n",
    "\n",
    "    return single_neural_net(x,weights,biases,i,sigma)*single_neural_net(x,weights,biases,j,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias helper function for quadrature\n",
    "def b_neural_net(x, weights, biases,i, sigma,f):\n",
    "    return single_neural_net(x,weights,biases,i, sigma)*f(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quadrature for the bias matrix - B\n",
    "def b_matrix(func,sigma, weights, biases,f, neurons):\n",
    "    b = np.zeros([neurons, 1])\n",
    "    for i in range(0, neurons):\n",
    "            b[i] = quad(func, 0, 1, args=(weights, biases,i,sigma,f), limit=1000)[0]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the approximation vector\n",
    "def u_nn(x, weights, biases, sigma,collocation, neurons, a):\n",
    "    u = np.zeros(collocation)\n",
    "    \n",
    "    for j in range(0,collocation):\n",
    "        for i in range(0,neurons):\n",
    "            \n",
    "            u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n",
    "        \n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for eigenvalues and eigenvectors\n",
    "from scipy.linalg import eig\n",
    "def get_eig(M):\n",
    "    D,V = eig(M, left=False, right=True)\n",
    "\n",
    "\n",
    "    idx = D.argsort()[::-1]   \n",
    "    D = D[idx]\n",
    "    V = V[:,idx]\n",
    "\n",
    "    D = np.flip(D)\n",
    "    V = np.fliplr(V)\n",
    "\n",
    "    return D,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate L2 loss\n",
    "def l2_loss(f_x, u_nn_result):\n",
    "    return np.linalg.norm(f_x - u_nn_result)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the Phi matrix, made up of n basis functions\n",
    "def phi_matrix(x,weights,biases,sigma,neurons,collocation):\n",
    "    phi = np.zeros((collocation,neurons))\n",
    "    for i in range(0,neurons):\n",
    "        for j in range(0,collocation):\n",
    "\n",
    "            phi[j,i] = sigma(weights[i]*x[j]+biases[i])\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=False):\n",
    "    phi = phi_matrix(x,weights,biases,sigma,neurons,collocation)\n",
    "    f_func = f(x)\n",
    "    a = np.linalg.pinv(phi)@f_func\n",
    "\n",
    "    u = u_nn(x, weights, biases, sigma, collocation,neurons, a)\n",
    "\n",
    "    loss = l2_loss(f_func, u)\n",
    "    if verbose:\n",
    "        print(\"=\"*20)\n",
    "        print(\"Sigma:\", sigma.__name__)\n",
    "        print(\"Number of collocation points:\", collocation)\n",
    "        print(\"Number of neurons:\", neurons)\n",
    "        print(\"L2 Loss:\", loss)\n",
    "        print(\"=\"*20)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_loss_values(neurons, collocations, sigmas, Rs,xmin=0,xmax=1,verbose=False):\n",
    "    loss_values = []\n",
    "    #np.random.seed(0)  # Set a seed for reproducibility\n",
    "    for sigma in sigmas:\n",
    "        for neuron in neurons:\n",
    "            for collocation in collocations:\n",
    "                for R in Rs:\n",
    "                    x = np.linspace(xmin, xmax, collocation)\n",
    "                    weights = np.random.uniform(-R, R, neuron)\n",
    "                    biases = np.random.uniform(-R, R, neuron)\n",
    "                    loss = evaluate_func_approx(x, f, weights, biases, sigma, neuron, collocation, verbose=verbose)\n",
    "                    loss_values.append((loss, collocation, neuron, sigma.__name__,R))  # Add activation function name\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def get_var_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def export_list(list,list_name,columns= [\"Loss\",\"# of Collocation\",\"# of Neurons\",\"Activation Function\",\"R - Sample Range\"]):\n",
    "    file_name = f\"{list_name}.xlsx\"\n",
    "    df = pd.DataFrame(list, columns=columns)\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Data exported to {file_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_collocation(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by collocation point, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each collocation point\n",
    "    collocation_loss_data = {collocation: [] for _, collocation, _, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (neuron, loss, activation_func_name, R) tuples for each collocation point\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        collocation_loss_data[collocation].append((neuron, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the collocation points and print the relevant data\n",
    "        for collocation, data in collocation_loss_data.items():\n",
    "            print(f\"# of Collocation points: {collocation}\")\n",
    "            \n",
    "            # Print the neurons, loss values, and R for the current activation function\n",
    "            for neuron, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Neurons: {neuron}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_neuron(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by neuron, activation function, and range R.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each neuron\n",
    "    neuron_loss_data = {neuron: [] for _, _, neuron, _, _ in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, loss, activation_func_name, R) tuples for each neuron\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        neuron_loss_data[neuron].append((collocation, loss, activation_func_name, R))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the neurons and print the relevant data\n",
    "        for neuron, data in neuron_loss_data.items():\n",
    "            print(f\"# of Neurons: {neuron}\")\n",
    "            \n",
    "            # Print the collocation points, loss values, and R for the current activation function\n",
    "            for collocation, loss, act_func_name, R in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, Loss: {loss:.10e}, R: {R}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_by_R(loss_values):\n",
    "    \"\"\"\n",
    "    Prints the loss values grouped by the range R, activation function, collocation points, and neurons.\n",
    "    \n",
    "    Parameters:\n",
    "    loss_values (list of tuples): Each tuple contains (loss, collocation, neuron, activation_func_name, R).\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store data for each R value\n",
    "    R_loss_data = {R: [] for _, _, _, _, R in loss_values}\n",
    "\n",
    "    # Populate the dictionary with (collocation, neuron, loss, activation_func_name) tuples for each R value\n",
    "    for loss, collocation, neuron, activation_func_name, R in loss_values:\n",
    "        R_loss_data[R].append((collocation, neuron, loss, activation_func_name))\n",
    "\n",
    "    # Iterate over the unique activation function names\n",
    "    for activation_func_name in set(activation_func_name for _, _, _, activation_func_name, _ in loss_values):\n",
    "        print(f\"Activation Function: {activation_func_name}\")\n",
    "        \n",
    "        # Iterate over the R values and print the relevant data\n",
    "        for R, data in R_loss_data.items():\n",
    "            print(f\"Range R: {R}\")\n",
    "            \n",
    "            # Print the collocation points, neurons, and loss values for the current activation function\n",
    "            for collocation, neuron, loss, act_func_name in data:\n",
    "                if act_func_name == activation_func_name:\n",
    "                    print(f\"  # of Collocation points: {collocation}, # of Neurons: {neuron}, Loss: {loss:.10e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Sigma: sin_function\n",
      "Number of collocation points: 100\n",
      "Number of neurons: 50\n",
      "L2 Loss: 0.08512083411934636\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "collocation = 100\n",
    "neurons = 50\n",
    "sigma = sin_function\n",
    "x = np.linspace(0,1,collocation)\n",
    "weights = np.random.uniform(-1,1,neurons)\n",
    "biases = np.random.uniform(-1,1,neurons)\n",
    "test = evaluate_func_approx(x,f, weights, biases, sigma, neurons, collocation,verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Increasing #of neurons\n",
    "Keeping collocations fixed at 50, and using ReLU, tanh, and sin as activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: relu_derivative\n",
      "Range R: 1\n",
      "  # of Collocation points: 50, # of Neurons: 5, Loss: 6.2505626656e+00\n",
      "  # of Collocation points: 50, # of Neurons: 10, Loss: 5.4622424458e+00\n",
      "  # of Collocation points: 50, # of Neurons: 20, Loss: 6.1745839338e+00\n",
      "  # of Collocation points: 50, # of Neurons: 30, Loss: 3.5907002681e+00\n",
      "  # of Collocation points: 50, # of Neurons: 40, Loss: 3.5916586565e+00\n",
      "  # of Collocation points: 50, # of Neurons: 50, Loss: 3.5743534093e+00\n",
      "  # of Collocation points: 50, # of Neurons: 100, Loss: 1.4128812006e+00\n",
      "Activation Function: sin_function_derivative\n",
      "Range R: 1\n",
      "  # of Collocation points: 50, # of Neurons: 5, Loss: 5.2540204831e+00\n",
      "  # of Collocation points: 50, # of Neurons: 10, Loss: 1.9421282965e-01\n",
      "  # of Collocation points: 50, # of Neurons: 20, Loss: 4.3044945804e-02\n",
      "  # of Collocation points: 50, # of Neurons: 30, Loss: 4.4020868158e-02\n",
      "  # of Collocation points: 50, # of Neurons: 40, Loss: 4.1909922053e-02\n",
      "  # of Collocation points: 50, # of Neurons: 50, Loss: 4.0228451798e-02\n",
      "  # of Collocation points: 50, # of Neurons: 100, Loss: 4.4283269030e-02\n",
      "Activation Function: tanh_derivative\n",
      "Range R: 1\n",
      "  # of Collocation points: 50, # of Neurons: 5, Loss: 5.3146193299e+00\n",
      "  # of Collocation points: 50, # of Neurons: 10, Loss: 7.5448011014e-02\n",
      "  # of Collocation points: 50, # of Neurons: 20, Loss: 6.8758568234e-05\n",
      "  # of Collocation points: 50, # of Neurons: 30, Loss: 7.8680226817e-08\n",
      "  # of Collocation points: 50, # of Neurons: 40, Loss: 4.7271162665e-05\n",
      "  # of Collocation points: 50, # of Neurons: 50, Loss: 4.9318679333e-05\n",
      "  # of Collocation points: 50, # of Neurons: 100, Loss: 1.9598899025e-03\n",
      "Data exported to loss_values_increasing_neurons.xlsx\n"
     ]
    }
   ],
   "source": [
    "neurons = [5,10,20,30,40,50,100]\n",
    "sigmas = [relu_derivative,tanh_derivative,sin_function_derivative]\n",
    "collocations = [50]\n",
    "Rs = [1]\n",
    "loss_values_increasing_neurons = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)\n",
    "print_loss_by_R(loss_values_increasing_neurons)\n",
    "#loss_values\n",
    "\n",
    "export_list(loss_values_increasing_neurons,\"loss_values_increasing_neurons\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - Increasing Collocation points\n",
    "Keeping neurons fixed at 20, and the same activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [20]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [10,50,100,200,500]\n",
    "Rs = [1]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: relu\n",
      "# of Neurons: 20\n",
      "  # of Collocation points: 10, Loss: 7.6679743793e-01, R: 1\n",
      "  # of Collocation points: 50, Loss: 2.7933344949e+00, R: 1\n",
      "  # of Collocation points: 100, Loss: 4.6528271642e+00, R: 1\n",
      "  # of Collocation points: 200, Loss: 2.1875022779e+01, R: 1\n",
      "  # of Collocation points: 500, Loss: 3.6592397331e+01, R: 1\n",
      "Activation Function: tanh\n",
      "# of Neurons: 20\n",
      "  # of Collocation points: 10, Loss: 7.5071934566e-14, R: 1\n",
      "  # of Collocation points: 50, Loss: 4.5618877679e-03, R: 1\n",
      "  # of Collocation points: 100, Loss: 3.7128314277e-03, R: 1\n",
      "  # of Collocation points: 200, Loss: 4.5068852383e-05, R: 1\n",
      "  # of Collocation points: 500, Loss: 3.9335319413e-05, R: 1\n",
      "Activation Function: sin_function\n",
      "# of Neurons: 20\n",
      "  # of Collocation points: 10, Loss: 1.9517762292e-05, R: 1\n",
      "  # of Collocation points: 50, Loss: 4.7273167519e-02, R: 1\n",
      "  # of Collocation points: 100, Loss: 8.1080991289e-02, R: 1\n",
      "  # of Collocation points: 200, Loss: 1.5959541909e-01, R: 1\n",
      "  # of Collocation points: 500, Loss: 3.9346536592e-01, R: 1\n"
     ]
    }
   ],
   "source": [
    "print_loss_by_neuron(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 - Increasing the sampling range R\n",
    "Keeping neurons and collocations fixed, increasing R to increase chance that w_i and b_i are further away from w_j and b_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [30]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "sigmas_derivative = [tanh_derivative,relu_derivative,sin_function_derivative]\n",
    "collocations = [50]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)\n",
    "derivative_loss_values = compute_loss_values(neurons, collocations,sigmas_derivative,Rs,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.2283148588909507e-06, 50, 30, 'tanh_derivative', 1),\n",
       " (0.0006388915575742714, 50, 30, 'tanh_derivative', 2),\n",
       " (1.491234810888612e-06, 50, 30, 'tanh_derivative', 4),\n",
       " (3.8311405099109073e-08, 50, 30, 'tanh_derivative', 5),\n",
       " (4.0555510274317083e-10, 50, 30, 'tanh_derivative', 10),\n",
       " (2.57785105051447, 50, 30, 'relu_derivative', 1),\n",
       " (3.63050588629014, 50, 30, 'relu_derivative', 2),\n",
       " (2.759521964803343, 50, 30, 'relu_derivative', 4),\n",
       " (3.769551114827367, 50, 30, 'relu_derivative', 5),\n",
       " (1.7786649807567088, 50, 30, 'relu_derivative', 10),\n",
       " (0.04309152254064139, 50, 30, 'sin_function_derivative', 1),\n",
       " (0.0008959539660297248, 50, 30, 'sin_function_derivative', 2),\n",
       " (1.946806745467099e-05, 50, 30, 'sin_function_derivative', 4),\n",
       " (0.0004994071518351346, 50, 30, 'sin_function_derivative', 5),\n",
       " (0.0013129232026776543, 50, 30, 'sin_function_derivative', 10)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: relu_derivative\n",
      "# of Collocation points: 50\n",
      "  # of Neurons: 30, Loss: 2.5778510505e+00, R: 1\n",
      "  # of Neurons: 30, Loss: 3.6305058863e+00, R: 2\n",
      "  # of Neurons: 30, Loss: 2.7595219648e+00, R: 4\n",
      "  # of Neurons: 30, Loss: 3.7695511148e+00, R: 5\n",
      "  # of Neurons: 30, Loss: 1.7786649808e+00, R: 10\n",
      "Activation Function: sin_function_derivative\n",
      "# of Collocation points: 50\n",
      "  # of Neurons: 30, Loss: 4.3091522541e-02, R: 1\n",
      "  # of Neurons: 30, Loss: 8.9595396603e-04, R: 2\n",
      "  # of Neurons: 30, Loss: 1.9468067455e-05, R: 4\n",
      "  # of Neurons: 30, Loss: 4.9940715184e-04, R: 5\n",
      "  # of Neurons: 30, Loss: 1.3129232027e-03, R: 10\n",
      "Activation Function: tanh_derivative\n",
      "# of Collocation points: 50\n",
      "  # of Neurons: 30, Loss: 3.2283148589e-06, R: 1\n",
      "  # of Neurons: 30, Loss: 6.3889155757e-04, R: 2\n",
      "  # of Neurons: 30, Loss: 1.4912348109e-06, R: 4\n",
      "  # of Neurons: 30, Loss: 3.8311405099e-08, R: 5\n",
      "  # of Neurons: 30, Loss: 4.0555510274e-10, R: 10\n"
     ]
    }
   ],
   "source": [
    "print_loss_by_collocation(derivative_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            R Value   Function Error      Derivative Error         \n",
      "================================================================================\n",
      "tanh                1         2.3434375050e-05    3.2283148589e-06         \n",
      "tanh                2         3.5857798345e-05    6.3889155757e-04         \n",
      "tanh                4         2.3862848033e-09    1.4912348109e-06         \n",
      "tanh                5         7.0521932236e-04    3.8311405099e-08         \n",
      "tanh                10        1.1318563900e-03    4.0555510274e-10         \n",
      "relu                1         1.1869997774e+00    2.5778510505e+00         \n",
      "relu                2         2.8164332146e+00    3.6305058863e+00         \n",
      "relu                4         4.5088958398e+00    2.7595219648e+00         \n",
      "relu                5         2.9962341889e+00    3.7695511148e+00         \n",
      "relu                10        4.0318911069e-01    1.7786649808e+00         \n",
      "sin_function        1         4.5290035084e-02    4.3091522541e-02         \n",
      "sin_function        2         8.5666752506e-04    8.9595396603e-04         \n",
      "sin_function        4         8.6480585223e-06    1.9468067455e-05         \n",
      "sin_function        5         8.6704699712e-06    4.9940715184e-04         \n",
      "sin_function        10        2.1662102256e-03    1.3129232027e-03         \n"
     ]
    }
   ],
   "source": [
    "function_dict = {(func, r): err for err, _, _, func, r in loss_values}\n",
    "derivative_dict = {(func, r): err for err, _, _, func, r in derivative_loss_values}\n",
    "\n",
    "def base_function_name(derivative_name):\n",
    "    if derivative_name.endswith('_derivative'):\n",
    "        return derivative_name.replace('_derivative', '')\n",
    "    return None\n",
    "\n",
    "comparison_results = []\n",
    "for (func_name, r), err in derivative_dict.items():\n",
    "    base_name = base_function_name(func_name)\n",
    "    if base_name:\n",
    "        base_err = function_dict.get((base_name, r))\n",
    "        if base_err is not None:\n",
    "            comparison_results.append((base_name, r, base_err, err))\n",
    "\n",
    "print(f\"{'Function':<20}{'R Value':<10}{'Function Error':<20}{'Derivative Error':<25}\")\n",
    "print(\"=\"*80)\n",
    "for base_name, r, base_err, der_err in comparison_results:\n",
    "    print(f\"{base_name:<20}{r:<10}{base_err:<20.10e}{der_err:<25.10e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [10,20,30,50,100]\n",
    "sigmas = [tanh,relu,sin_function]\n",
    "collocations = [5,10,30,50,100]\n",
    "Rs = [1,2,4,5,10]\n",
    "loss_values = compute_loss_values(neurons, collocations,sigmas,Rs,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: relu\n",
      "# of Collocation points: 5\n",
      "  # of Neurons: 10, Loss: 3.7319916643e-01, R: 1\n",
      "  # of Neurons: 10, Loss: 1.7828256458e-28, R: 2\n",
      "  # of Neurons: 10, Loss: 7.8874320223e-01, R: 4\n",
      "  # of Neurons: 10, Loss: 3.7319916643e-01, R: 5\n",
      "  # of Neurons: 10, Loss: 8.6764838813e-27, R: 10\n",
      "  # of Neurons: 20, Loss: 9.5649384758e-29, R: 1\n",
      "  # of Neurons: 20, Loss: 1.3459939195e-29, R: 2\n",
      "  # of Neurons: 20, Loss: 1.8143800820e-28, R: 4\n",
      "  # of Neurons: 20, Loss: 3.6022408673e-01, R: 5\n",
      "  # of Neurons: 20, Loss: 3.5893171188e-29, R: 10\n",
      "  # of Neurons: 30, Loss: 2.9631587752e-29, R: 1\n",
      "  # of Neurons: 30, Loss: 3.1554436209e-29, R: 2\n",
      "  # of Neurons: 30, Loss: 1.3016204936e-29, R: 4\n",
      "  # of Neurons: 30, Loss: 3.6188994027e-29, R: 5\n",
      "  # of Neurons: 30, Loss: 1.0649622220e-29, R: 10\n",
      "  # of Neurons: 50, Loss: 8.0266597106e-29, R: 1\n",
      "  # of Neurons: 50, Loss: 5.1275958839e-30, R: 2\n",
      "  # of Neurons: 50, Loss: 3.1061398143e-28, R: 4\n",
      "  # of Neurons: 50, Loss: 1.4002281068e-29, R: 5\n",
      "  # of Neurons: 50, Loss: 1.7157724689e-29, R: 10\n",
      "  # of Neurons: 100, Loss: 6.5574062746e-30, R: 1\n",
      "  # of Neurons: 100, Loss: 1.2227344031e-29, R: 2\n",
      "  # of Neurons: 100, Loss: 1.7354939915e-29, R: 4\n",
      "  # of Neurons: 100, Loss: 1.4396711520e-29, R: 5\n",
      "  # of Neurons: 100, Loss: 1.9376395984e-29, R: 10\n",
      "# of Collocation points: 10\n",
      "  # of Neurons: 10, Loss: 8.3290310563e-01, R: 1\n",
      "  # of Neurons: 10, Loss: 8.6564794508e-01, R: 2\n",
      "  # of Neurons: 10, Loss: 1.3250396147e+00, R: 4\n",
      "  # of Neurons: 10, Loss: 7.9934515819e-01, R: 5\n",
      "  # of Neurons: 10, Loss: 3.2783222472e-01, R: 10\n",
      "  # of Neurons: 20, Loss: 7.1581733488e-01, R: 1\n",
      "  # of Neurons: 20, Loss: 1.0350180816e+00, R: 2\n",
      "  # of Neurons: 20, Loss: 1.5775686825e-01, R: 4\n",
      "  # of Neurons: 20, Loss: 6.3870536745e-01, R: 5\n",
      "  # of Neurons: 20, Loss: 4.3183864388e-01, R: 10\n",
      "  # of Neurons: 30, Loss: 1.0206133983e-02, R: 1\n",
      "  # of Neurons: 30, Loss: 8.3240047562e-01, R: 2\n",
      "  # of Neurons: 30, Loss: 1.7277901175e-01, R: 4\n",
      "  # of Neurons: 30, Loss: 5.8950296480e-01, R: 5\n",
      "  # of Neurons: 30, Loss: 7.6085349073e-01, R: 10\n",
      "  # of Neurons: 50, Loss: 2.6261672573e-27, R: 1\n",
      "  # of Neurons: 50, Loss: 8.0000675042e-01, R: 2\n",
      "  # of Neurons: 50, Loss: 3.3416799260e-01, R: 4\n",
      "  # of Neurons: 50, Loss: 2.9010852828e-27, R: 5\n",
      "  # of Neurons: 50, Loss: 6.9333943426e-24, R: 10\n",
      "  # of Neurons: 100, Loss: 2.6245895355e-27, R: 1\n",
      "  # of Neurons: 100, Loss: 1.6299838454e-28, R: 2\n",
      "  # of Neurons: 100, Loss: 6.0781732747e-28, R: 4\n",
      "  # of Neurons: 100, Loss: 2.8734258473e-28, R: 5\n",
      "  # of Neurons: 100, Loss: 3.5483949593e-28, R: 10\n",
      "# of Collocation points: 30\n",
      "  # of Neurons: 10, Loss: 2.8277232321e+00, R: 1\n",
      "  # of Neurons: 10, Loss: 4.1781972174e+00, R: 2\n",
      "  # of Neurons: 10, Loss: 2.9814018594e+00, R: 4\n",
      "  # of Neurons: 10, Loss: 3.1917473201e+00, R: 5\n",
      "  # of Neurons: 10, Loss: 2.8300765949e+00, R: 10\n",
      "  # of Neurons: 20, Loss: 2.9054275822e+00, R: 1\n",
      "  # of Neurons: 20, Loss: 2.1740182581e-01, R: 2\n",
      "  # of Neurons: 20, Loss: 1.3981327150e+00, R: 4\n",
      "  # of Neurons: 20, Loss: 2.3794237976e+00, R: 5\n",
      "  # of Neurons: 20, Loss: 1.9614219983e+00, R: 10\n",
      "  # of Neurons: 30, Loss: 5.8814288926e-01, R: 1\n",
      "  # of Neurons: 30, Loss: 3.8409356841e-01, R: 2\n",
      "  # of Neurons: 30, Loss: 1.1897436721e+00, R: 4\n",
      "  # of Neurons: 30, Loss: 1.0826795630e+00, R: 5\n",
      "  # of Neurons: 30, Loss: 1.7149045608e+00, R: 10\n",
      "  # of Neurons: 50, Loss: 1.0099673216e+00, R: 1\n",
      "  # of Neurons: 50, Loss: 1.0454021884e-01, R: 2\n",
      "  # of Neurons: 50, Loss: 4.9107338018e-01, R: 4\n",
      "  # of Neurons: 50, Loss: 9.0384586220e-02, R: 5\n",
      "  # of Neurons: 50, Loss: 6.0864370165e-01, R: 10\n",
      "  # of Neurons: 100, Loss: 1.0411446035e-02, R: 1\n",
      "  # of Neurons: 100, Loss: 2.9975853454e-02, R: 2\n",
      "  # of Neurons: 100, Loss: 2.9287650218e-03, R: 4\n",
      "  # of Neurons: 100, Loss: 2.7815575104e-02, R: 5\n",
      "  # of Neurons: 100, Loss: 2.2394868811e-02, R: 10\n",
      "# of Collocation points: 50\n",
      "  # of Neurons: 10, Loss: 5.8387401438e+00, R: 1\n",
      "  # of Neurons: 10, Loss: 4.5303349642e+00, R: 2\n",
      "  # of Neurons: 10, Loss: 4.4430344461e+00, R: 4\n",
      "  # of Neurons: 10, Loss: 4.3099078658e+00, R: 5\n",
      "  # of Neurons: 10, Loss: 4.9914218214e+00, R: 10\n",
      "  # of Neurons: 20, Loss: 1.4661624660e+00, R: 1\n",
      "  # of Neurons: 20, Loss: 5.9894077608e-01, R: 2\n",
      "  # of Neurons: 20, Loss: 5.1325756032e+00, R: 4\n",
      "  # of Neurons: 20, Loss: 4.4997554385e+00, R: 5\n",
      "  # of Neurons: 20, Loss: 2.0966378642e+00, R: 10\n",
      "  # of Neurons: 30, Loss: 1.1957999582e+00, R: 1\n",
      "  # of Neurons: 30, Loss: 7.7772590017e-01, R: 2\n",
      "  # of Neurons: 30, Loss: 1.8593183696e+00, R: 4\n",
      "  # of Neurons: 30, Loss: 2.0091812729e+00, R: 5\n",
      "  # of Neurons: 30, Loss: 1.3020894354e+00, R: 10\n",
      "  # of Neurons: 50, Loss: 1.6238639577e+00, R: 1\n",
      "  # of Neurons: 50, Loss: 6.0978846236e-01, R: 2\n",
      "  # of Neurons: 50, Loss: 8.7060523680e-01, R: 4\n",
      "  # of Neurons: 50, Loss: 8.2742874488e-01, R: 5\n",
      "  # of Neurons: 50, Loss: 1.0340894167e+00, R: 10\n",
      "  # of Neurons: 100, Loss: 3.5807052423e-02, R: 1\n",
      "  # of Neurons: 100, Loss: 7.5180346497e-02, R: 2\n",
      "  # of Neurons: 100, Loss: 2.8048788286e-02, R: 4\n",
      "  # of Neurons: 100, Loss: 5.1379577251e-02, R: 5\n",
      "  # of Neurons: 100, Loss: 2.4624058296e-01, R: 10\n",
      "# of Collocation points: 100\n",
      "  # of Neurons: 10, Loss: 1.0148601681e+01, R: 1\n",
      "  # of Neurons: 10, Loss: 9.0670645024e+00, R: 2\n",
      "  # of Neurons: 10, Loss: 1.0673548835e+01, R: 4\n",
      "  # of Neurons: 10, Loss: 1.1672785145e+01, R: 5\n",
      "  # of Neurons: 10, Loss: 1.0613263353e+01, R: 10\n",
      "  # of Neurons: 20, Loss: 9.9732312723e+00, R: 1\n",
      "  # of Neurons: 20, Loss: 9.2228661850e+00, R: 2\n",
      "  # of Neurons: 20, Loss: 1.0234128919e+01, R: 4\n",
      "  # of Neurons: 20, Loss: 1.0460518882e+00, R: 5\n",
      "  # of Neurons: 20, Loss: 9.1443103074e+00, R: 10\n",
      "  # of Neurons: 30, Loss: 1.6458452376e+00, R: 1\n",
      "  # of Neurons: 30, Loss: 6.8399183941e+00, R: 2\n",
      "  # of Neurons: 30, Loss: 4.9442311495e+00, R: 4\n",
      "  # of Neurons: 30, Loss: 5.2700504827e+00, R: 5\n",
      "  # of Neurons: 30, Loss: 5.7217719539e+00, R: 10\n",
      "  # of Neurons: 50, Loss: 4.5665785893e+00, R: 1\n",
      "  # of Neurons: 50, Loss: 9.1813216857e-01, R: 2\n",
      "  # of Neurons: 50, Loss: 3.3106613224e-01, R: 4\n",
      "  # of Neurons: 50, Loss: 1.6874655538e+00, R: 5\n",
      "  # of Neurons: 50, Loss: 4.2215841513e+00, R: 10\n",
      "  # of Neurons: 100, Loss: 1.4529822532e-01, R: 1\n",
      "  # of Neurons: 100, Loss: 2.5221929836e-01, R: 2\n",
      "  # of Neurons: 100, Loss: 3.6745322735e-01, R: 4\n",
      "  # of Neurons: 100, Loss: 2.7251907917e-02, R: 5\n",
      "  # of Neurons: 100, Loss: 3.8747453833e-01, R: 10\n",
      "Activation Function: tanh\n",
      "# of Collocation points: 5\n",
      "  # of Neurons: 10, Loss: 5.4124172692e-24, R: 1\n",
      "  # of Neurons: 10, Loss: 9.7708311721e-27, R: 2\n",
      "  # of Neurons: 10, Loss: 2.2334624379e-29, R: 4\n",
      "  # of Neurons: 10, Loss: 1.1300432467e-28, R: 5\n",
      "  # of Neurons: 10, Loss: 2.7613717578e-20, R: 10\n",
      "  # of Neurons: 20, Loss: 2.0363813180e-25, R: 1\n",
      "  # of Neurons: 20, Loss: 1.8045982068e-26, R: 2\n",
      "  # of Neurons: 20, Loss: 5.1197072749e-28, R: 4\n",
      "  # of Neurons: 20, Loss: 1.2286508599e-28, R: 5\n",
      "  # of Neurons: 20, Loss: 2.1200636828e-29, R: 10\n",
      "  # of Neurons: 30, Loss: 1.2173811930e-24, R: 1\n",
      "  # of Neurons: 30, Loss: 1.5761440886e-27, R: 2\n",
      "  # of Neurons: 30, Loss: 1.7650762754e-28, R: 4\n",
      "  # of Neurons: 30, Loss: 1.1798400914e-28, R: 5\n",
      "  # of Neurons: 30, Loss: 2.8596207814e-30, R: 10\n",
      "  # of Neurons: 50, Loss: 4.0747978970e-25, R: 1\n",
      "  # of Neurons: 50, Loss: 1.5117533172e-27, R: 2\n",
      "  # of Neurons: 50, Loss: 7.0011405338e-30, R: 4\n",
      "  # of Neurons: 50, Loss: 3.1751651435e-29, R: 5\n",
      "  # of Neurons: 50, Loss: 3.1554436209e-30, R: 10\n",
      "  # of Neurons: 100, Loss: 8.3038900846e-26, R: 1\n",
      "  # of Neurons: 100, Loss: 9.5136625170e-28, R: 2\n",
      "  # of Neurons: 100, Loss: 7.2624507087e-29, R: 4\n",
      "  # of Neurons: 100, Loss: 1.6812598043e-29, R: 5\n",
      "  # of Neurons: 100, Loss: 3.6139690220e-29, R: 10\n",
      "# of Collocation points: 10\n",
      "  # of Neurons: 10, Loss: 5.0761160287e-10, R: 1\n",
      "  # of Neurons: 10, Loss: 4.7111750358e-11, R: 2\n",
      "  # of Neurons: 10, Loss: 2.8268548540e-16, R: 4\n",
      "  # of Neurons: 10, Loss: 7.9308750226e-15, R: 5\n",
      "  # of Neurons: 10, Loss: 8.0375689830e-10, R: 10\n",
      "  # of Neurons: 20, Loss: 1.0031288236e-13, R: 1\n",
      "  # of Neurons: 20, Loss: 2.5059394449e-19, R: 2\n",
      "  # of Neurons: 20, Loss: 4.5926193258e-20, R: 4\n",
      "  # of Neurons: 20, Loss: 1.2858540932e-21, R: 5\n",
      "  # of Neurons: 20, Loss: 4.3181289261e-23, R: 10\n",
      "  # of Neurons: 30, Loss: 1.4085537725e-14, R: 1\n",
      "  # of Neurons: 30, Loss: 7.4650190504e-18, R: 2\n",
      "  # of Neurons: 30, Loss: 1.3512390991e-22, R: 4\n",
      "  # of Neurons: 30, Loss: 3.8909378837e-23, R: 5\n",
      "  # of Neurons: 30, Loss: 3.1290407738e-22, R: 10\n",
      "  # of Neurons: 50, Loss: 2.4489472272e-15, R: 1\n",
      "  # of Neurons: 50, Loss: 1.4049474396e-19, R: 2\n",
      "  # of Neurons: 50, Loss: 3.9451288858e-25, R: 4\n",
      "  # of Neurons: 50, Loss: 2.1931320227e-24, R: 5\n",
      "  # of Neurons: 50, Loss: 4.3604286536e-28, R: 10\n",
      "  # of Neurons: 100, Loss: 2.3139039756e-15, R: 1\n",
      "  # of Neurons: 100, Loss: 8.6889123929e-20, R: 2\n",
      "  # of Neurons: 100, Loss: 1.3117475448e-24, R: 4\n",
      "  # of Neurons: 100, Loss: 4.2045102957e-26, R: 5\n",
      "  # of Neurons: 100, Loss: 1.4129484889e-27, R: 10\n",
      "# of Collocation points: 30\n",
      "  # of Neurons: 10, Loss: 5.1674482682e-02, R: 1\n",
      "  # of Neurons: 10, Loss: 2.3798323611e-02, R: 2\n",
      "  # of Neurons: 10, Loss: 5.5596760178e-02, R: 4\n",
      "  # of Neurons: 10, Loss: 9.1044305656e-03, R: 5\n",
      "  # of Neurons: 10, Loss: 1.6101145616e-01, R: 10\n",
      "  # of Neurons: 20, Loss: 9.9072880879e-05, R: 1\n",
      "  # of Neurons: 20, Loss: 5.8602689996e-05, R: 2\n",
      "  # of Neurons: 20, Loss: 4.7784286192e-04, R: 4\n",
      "  # of Neurons: 20, Loss: 6.1697597180e-04, R: 5\n",
      "  # of Neurons: 20, Loss: 4.4900872082e-03, R: 10\n",
      "  # of Neurons: 30, Loss: 1.3255964782e-04, R: 1\n",
      "  # of Neurons: 30, Loss: 3.7357153167e-04, R: 2\n",
      "  # of Neurons: 30, Loss: 3.1652882527e-06, R: 4\n",
      "  # of Neurons: 30, Loss: 5.5027656568e-07, R: 5\n",
      "  # of Neurons: 30, Loss: 5.7581779849e-05, R: 10\n",
      "  # of Neurons: 50, Loss: 9.0954837731e-05, R: 1\n",
      "  # of Neurons: 50, Loss: 4.7952165367e-03, R: 2\n",
      "  # of Neurons: 50, Loss: 1.6224690858e-05, R: 4\n",
      "  # of Neurons: 50, Loss: 5.7021462227e-06, R: 5\n",
      "  # of Neurons: 50, Loss: 1.7939896132e-05, R: 10\n",
      "  # of Neurons: 100, Loss: 6.2739037126e-07, R: 1\n",
      "  # of Neurons: 100, Loss: 5.3349970112e-06, R: 2\n",
      "  # of Neurons: 100, Loss: 7.8955346367e-07, R: 4\n",
      "  # of Neurons: 100, Loss: 1.0408092116e-03, R: 5\n",
      "  # of Neurons: 100, Loss: 1.5997107888e-12, R: 10\n",
      "# of Collocation points: 50\n",
      "  # of Neurons: 10, Loss: 1.1907122909e-01, R: 1\n",
      "  # of Neurons: 10, Loss: 1.9345577072e-02, R: 2\n",
      "  # of Neurons: 10, Loss: 3.1988241392e-02, R: 4\n",
      "  # of Neurons: 10, Loss: 6.3374896276e-02, R: 5\n",
      "  # of Neurons: 10, Loss: 4.1774323357e-01, R: 10\n",
      "  # of Neurons: 20, Loss: 6.0775861031e-06, R: 1\n",
      "  # of Neurons: 20, Loss: 7.2358624307e-05, R: 2\n",
      "  # of Neurons: 20, Loss: 1.8882218134e-03, R: 4\n",
      "  # of Neurons: 20, Loss: 6.5740610568e-06, R: 5\n",
      "  # of Neurons: 20, Loss: 8.7704051273e-06, R: 10\n",
      "  # of Neurons: 30, Loss: 3.5230578232e-04, R: 1\n",
      "  # of Neurons: 30, Loss: 2.4236873868e-03, R: 2\n",
      "  # of Neurons: 30, Loss: 1.0683572272e-05, R: 4\n",
      "  # of Neurons: 30, Loss: 9.6172563864e-04, R: 5\n",
      "  # of Neurons: 30, Loss: 6.0518858396e-07, R: 10\n",
      "  # of Neurons: 50, Loss: 2.2198799825e-06, R: 1\n",
      "  # of Neurons: 50, Loss: 1.2146843929e-05, R: 2\n",
      "  # of Neurons: 50, Loss: 2.7841630551e-05, R: 4\n",
      "  # of Neurons: 50, Loss: 7.1076122243e-04, R: 5\n",
      "  # of Neurons: 50, Loss: 1.3947279673e-03, R: 10\n",
      "  # of Neurons: 100, Loss: 4.8063026329e-05, R: 1\n",
      "  # of Neurons: 100, Loss: 2.5010108247e-06, R: 2\n",
      "  # of Neurons: 100, Loss: 1.8016942911e-05, R: 4\n",
      "  # of Neurons: 100, Loss: 1.5670505959e-04, R: 5\n",
      "  # of Neurons: 100, Loss: 8.2951512871e-05, R: 10\n",
      "# of Collocation points: 100\n",
      "  # of Neurons: 10, Loss: 7.9728341514e-02, R: 1\n",
      "  # of Neurons: 10, Loss: 2.4806914224e-01, R: 2\n",
      "  # of Neurons: 10, Loss: 5.6370299719e-01, R: 4\n",
      "  # of Neurons: 10, Loss: 2.0884133438e-01, R: 5\n",
      "  # of Neurons: 10, Loss: 1.1545986078e-01, R: 10\n",
      "  # of Neurons: 20, Loss: 6.9671181230e-05, R: 1\n",
      "  # of Neurons: 20, Loss: 1.7529680035e-07, R: 2\n",
      "  # of Neurons: 20, Loss: 4.0455557224e-03, R: 4\n",
      "  # of Neurons: 20, Loss: 3.2323316909e-04, R: 5\n",
      "  # of Neurons: 20, Loss: 6.6374926656e-04, R: 10\n",
      "  # of Neurons: 30, Loss: 7.8047213370e-05, R: 1\n",
      "  # of Neurons: 30, Loss: 2.6034070533e-05, R: 2\n",
      "  # of Neurons: 30, Loss: 7.9408220425e-07, R: 4\n",
      "  # of Neurons: 30, Loss: 4.0106144251e-05, R: 5\n",
      "  # of Neurons: 30, Loss: 4.0195142722e-05, R: 10\n",
      "  # of Neurons: 50, Loss: 1.4193388916e-04, R: 1\n",
      "  # of Neurons: 50, Loss: 7.0700130107e-04, R: 2\n",
      "  # of Neurons: 50, Loss: 7.8159595359e-05, R: 4\n",
      "  # of Neurons: 50, Loss: 1.1282217899e-05, R: 5\n",
      "  # of Neurons: 50, Loss: 2.2446212286e-05, R: 10\n",
      "  # of Neurons: 100, Loss: 5.7329404156e-07, R: 1\n",
      "  # of Neurons: 100, Loss: 1.3667056293e-05, R: 2\n",
      "  # of Neurons: 100, Loss: 2.1277715408e-05, R: 4\n",
      "  # of Neurons: 100, Loss: 2.4826465985e-05, R: 5\n",
      "  # of Neurons: 100, Loss: 6.1579593099e-04, R: 10\n",
      "Activation Function: sin_function\n",
      "# of Collocation points: 5\n",
      "  # of Neurons: 10, Loss: 1.3018931981e-21, R: 1\n",
      "  # of Neurons: 10, Loss: 8.1399559138e-25, R: 2\n",
      "  # of Neurons: 10, Loss: 3.2718006044e-28, R: 4\n",
      "  # of Neurons: 10, Loss: 7.9438293156e-28, R: 5\n",
      "  # of Neurons: 10, Loss: 7.3462671799e-30, R: 10\n",
      "  # of Neurons: 20, Loss: 3.0322789255e-23, R: 1\n",
      "  # of Neurons: 20, Loss: 1.8272385148e-25, R: 2\n",
      "  # of Neurons: 20, Loss: 6.2172100093e-29, R: 4\n",
      "  # of Neurons: 20, Loss: 3.7076462545e-29, R: 5\n",
      "  # of Neurons: 20, Loss: 2.8645511621e-29, R: 10\n",
      "  # of Neurons: 30, Loss: 7.3981545059e-23, R: 1\n",
      "  # of Neurons: 30, Loss: 3.2789003526e-25, R: 2\n",
      "  # of Neurons: 30, Loss: 7.5888419082e-28, R: 4\n",
      "  # of Neurons: 30, Loss: 5.5713301431e-30, R: 5\n",
      "  # of Neurons: 30, Loss: 9.2691156363e-30, R: 10\n",
      "  # of Neurons: 50, Loss: 1.3112493298e-24, R: 1\n",
      "  # of Neurons: 50, Loss: 3.1830537526e-26, R: 2\n",
      "  # of Neurons: 50, Loss: 7.2377988054e-29, R: 4\n",
      "  # of Neurons: 50, Loss: 3.1948866661e-29, R: 5\n",
      "  # of Neurons: 50, Loss: 1.7650762754e-29, R: 10\n",
      "  # of Neurons: 100, Loss: 6.8703331963e-24, R: 1\n",
      "  # of Neurons: 100, Loss: 1.7309185982e-26, R: 2\n",
      "  # of Neurons: 100, Loss: 2.1008351982e-28, R: 4\n",
      "  # of Neurons: 100, Loss: 7.2821722313e-29, R: 5\n",
      "  # of Neurons: 100, Loss: 1.1931521191e-29, R: 10\n",
      "# of Collocation points: 10\n",
      "  # of Neurons: 10, Loss: 1.0438039043e-02, R: 1\n",
      "  # of Neurons: 10, Loss: 7.0180934395e-09, R: 2\n",
      "  # of Neurons: 10, Loss: 1.0106446404e-12, R: 4\n",
      "  # of Neurons: 10, Loss: 1.3646119665e-16, R: 5\n",
      "  # of Neurons: 10, Loss: 3.9568589536e-21, R: 10\n",
      "  # of Neurons: 20, Loss: 1.4552384476e-06, R: 1\n",
      "  # of Neurons: 20, Loss: 1.3640219528e-09, R: 2\n",
      "  # of Neurons: 20, Loss: 1.5367765162e-16, R: 4\n",
      "  # of Neurons: 20, Loss: 3.1244036894e-16, R: 5\n",
      "  # of Neurons: 20, Loss: 3.8820838716e-23, R: 10\n",
      "  # of Neurons: 30, Loss: 2.8971462168e-05, R: 1\n",
      "  # of Neurons: 30, Loss: 4.2266463663e-10, R: 2\n",
      "  # of Neurons: 30, Loss: 2.4423449232e-16, R: 4\n",
      "  # of Neurons: 30, Loss: 5.5107363087e-17, R: 5\n",
      "  # of Neurons: 30, Loss: 1.2222385448e-23, R: 10\n",
      "  # of Neurons: 50, Loss: 4.4712660434e-05, R: 1\n",
      "  # of Neurons: 50, Loss: 1.3852990428e-11, R: 2\n",
      "  # of Neurons: 50, Loss: 3.0459088263e-15, R: 4\n",
      "  # of Neurons: 50, Loss: 1.5683617448e-17, R: 5\n",
      "  # of Neurons: 50, Loss: 3.9897411393e-24, R: 10\n",
      "  # of Neurons: 100, Loss: 1.4832952247e-05, R: 1\n",
      "  # of Neurons: 100, Loss: 3.4703112591e-11, R: 2\n",
      "  # of Neurons: 100, Loss: 1.1625944987e-16, R: 4\n",
      "  # of Neurons: 100, Loss: 1.3428264096e-18, R: 5\n",
      "  # of Neurons: 100, Loss: 6.2565183565e-24, R: 10\n",
      "# of Collocation points: 30\n",
      "  # of Neurons: 10, Loss: 1.2078269676e-01, R: 1\n",
      "  # of Neurons: 10, Loss: 2.3090482493e-02, R: 2\n",
      "  # of Neurons: 10, Loss: 3.6213390605e-02, R: 4\n",
      "  # of Neurons: 10, Loss: 2.4288785332e-02, R: 5\n",
      "  # of Neurons: 10, Loss: 2.1380118220e-02, R: 10\n",
      "  # of Neurons: 20, Loss: 2.6373868720e-02, R: 1\n",
      "  # of Neurons: 20, Loss: 5.3852158923e-04, R: 2\n",
      "  # of Neurons: 20, Loss: 3.1155843559e-06, R: 4\n",
      "  # of Neurons: 20, Loss: 8.6655435589e-05, R: 5\n",
      "  # of Neurons: 20, Loss: 3.3348575687e-04, R: 10\n",
      "  # of Neurons: 30, Loss: 2.7113867637e-02, R: 1\n",
      "  # of Neurons: 30, Loss: 4.8116092434e-04, R: 2\n",
      "  # of Neurons: 30, Loss: 3.1421225598e-04, R: 4\n",
      "  # of Neurons: 30, Loss: 3.9454446032e-06, R: 5\n",
      "  # of Neurons: 30, Loss: 5.5206239775e-03, R: 10\n",
      "  # of Neurons: 50, Loss: 2.6903026661e-02, R: 1\n",
      "  # of Neurons: 50, Loss: 5.2978512932e-04, R: 2\n",
      "  # of Neurons: 50, Loss: 3.1819906681e-06, R: 4\n",
      "  # of Neurons: 50, Loss: 9.4642720027e-07, R: 5\n",
      "  # of Neurons: 50, Loss: 2.3515043503e-04, R: 10\n",
      "  # of Neurons: 100, Loss: 2.7113267101e-02, R: 1\n",
      "  # of Neurons: 100, Loss: 4.4981768384e-04, R: 2\n",
      "  # of Neurons: 100, Loss: 3.2975750936e-06, R: 4\n",
      "  # of Neurons: 100, Loss: 3.0129727672e-06, R: 5\n",
      "  # of Neurons: 100, Loss: 7.5060786293e-05, R: 10\n",
      "# of Collocation points: 50\n",
      "  # of Neurons: 10, Loss: 4.4383364677e-02, R: 1\n",
      "  # of Neurons: 10, Loss: 5.7475344988e-02, R: 2\n",
      "  # of Neurons: 10, Loss: 5.4417178433e-02, R: 4\n",
      "  # of Neurons: 10, Loss: 4.3186350867e-02, R: 5\n",
      "  # of Neurons: 10, Loss: 1.1827952678e-01, R: 10\n",
      "  # of Neurons: 20, Loss: 4.5588835449e-02, R: 1\n",
      "  # of Neurons: 20, Loss: 8.7237679089e-04, R: 2\n",
      "  # of Neurons: 20, Loss: 6.6769091801e-05, R: 4\n",
      "  # of Neurons: 20, Loss: 1.0874480918e-04, R: 5\n",
      "  # of Neurons: 20, Loss: 7.6117632460e-06, R: 10\n",
      "  # of Neurons: 30, Loss: 4.4285713608e-02, R: 1\n",
      "  # of Neurons: 30, Loss: 8.3582356473e-04, R: 2\n",
      "  # of Neurons: 30, Loss: 1.7295639450e-05, R: 4\n",
      "  # of Neurons: 30, Loss: 5.8874602923e-06, R: 5\n",
      "  # of Neurons: 30, Loss: 1.3033063070e-05, R: 10\n",
      "  # of Neurons: 50, Loss: 4.5330258377e-02, R: 1\n",
      "  # of Neurons: 50, Loss: 8.8455686147e-04, R: 2\n",
      "  # of Neurons: 50, Loss: 1.4170502684e-05, R: 4\n",
      "  # of Neurons: 50, Loss: 1.0404961977e-05, R: 5\n",
      "  # of Neurons: 50, Loss: 9.7107803589e-05, R: 10\n",
      "  # of Neurons: 100, Loss: 4.5418908382e-02, R: 1\n",
      "  # of Neurons: 100, Loss: 8.7046440610e-04, R: 2\n",
      "  # of Neurons: 100, Loss: 1.2821585799e-04, R: 4\n",
      "  # of Neurons: 100, Loss: 5.7047861393e-07, R: 5\n",
      "  # of Neurons: 100, Loss: 8.6160668412e-06, R: 10\n",
      "# of Collocation points: 100\n",
      "  # of Neurons: 10, Loss: 2.3572241959e-01, R: 1\n",
      "  # of Neurons: 10, Loss: 9.5329483711e-02, R: 2\n",
      "  # of Neurons: 10, Loss: 7.6649637308e-02, R: 4\n",
      "  # of Neurons: 10, Loss: 3.3585758500e-02, R: 5\n",
      "  # of Neurons: 10, Loss: 2.1038557935e-02, R: 10\n",
      "  # of Neurons: 20, Loss: 8.1909800802e-02, R: 1\n",
      "  # of Neurons: 20, Loss: 1.5643926578e-03, R: 2\n",
      "  # of Neurons: 20, Loss: 3.6978590963e-04, R: 4\n",
      "  # of Neurons: 20, Loss: 1.2773610880e-05, R: 5\n",
      "  # of Neurons: 20, Loss: 8.2832898134e-04, R: 10\n",
      "  # of Neurons: 30, Loss: 8.4959162249e-02, R: 1\n",
      "  # of Neurons: 30, Loss: 1.7852859890e-03, R: 2\n",
      "  # of Neurons: 30, Loss: 2.3672948123e-05, R: 4\n",
      "  # of Neurons: 30, Loss: 2.5251256850e-05, R: 5\n",
      "  # of Neurons: 30, Loss: 3.2911914323e-04, R: 10\n",
      "  # of Neurons: 50, Loss: 8.5868100758e-02, R: 1\n",
      "  # of Neurons: 50, Loss: 1.6989657252e-03, R: 2\n",
      "  # of Neurons: 50, Loss: 1.5432694903e-05, R: 4\n",
      "  # of Neurons: 50, Loss: 1.2312788568e-05, R: 5\n",
      "  # of Neurons: 50, Loss: 2.9066222569e-05, R: 10\n",
      "  # of Neurons: 100, Loss: 8.5712925724e-02, R: 1\n",
      "  # of Neurons: 100, Loss: 1.6263024981e-03, R: 2\n",
      "  # of Neurons: 100, Loss: 1.3376453423e-05, R: 4\n",
      "  # of Neurons: 100, Loss: 1.4101250321e-04, R: 5\n",
      "  # of Neurons: 100, Loss: 8.0863285234e-06, R: 10\n"
     ]
    }
   ],
   "source": [
    "print_loss_by_collocation(loss_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def mass_matrix(func,sigma, weights, biases, neurons):\n",
    "    M = np.zeros([neurons, neurons])\n",
    "    \n",
    "    for i in range(0, neurons):\n",
    "        for j in range(0, neurons):\n",
    "            M[i, j] = quad(func, 0, 1, args=(weights, biases,i,j,sigma), limit=1000)[0]\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 10\n",
    "R = 10\n",
    "weights = np.random.uniform(-R,R,neurons)\n",
    "biases = np.random.uniform(-R,R,neurons)\n",
    "sigma = tanh_derivative\n",
    "M = mass_matrix(double_neural_net,sigma, weights, biases, neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.14083371e-04, 2.63341331e-05, 2.92212164e-03, 2.00910030e-09,\n",
       "        4.47134804e-08, 6.01008130e-03, 3.49943224e-06, 7.82125665e-03,\n",
       "        6.05666037e-05, 1.81567508e-09],\n",
       "       [2.63341331e-05, 1.13340412e-01, 5.20604622e-03, 9.19080077e-07,\n",
       "        1.24227430e-08, 1.70907484e-04, 3.46374235e-02, 5.19942089e-04,\n",
       "        1.29322006e-01, 1.37987206e-06],\n",
       "       [2.92212164e-03, 5.20604622e-03, 2.31868604e-01, 2.06402451e-07,\n",
       "        2.94663154e-07, 2.36118249e-02, 5.00618247e-04, 4.05127345e-02,\n",
       "        1.36342837e-02, 2.25756872e-07],\n",
       "       [2.00910030e-09, 9.19080077e-07, 2.06402451e-07, 7.88375438e-12,\n",
       "        3.00637122e-13, 1.54471169e-08, 2.76961657e-07, 2.84146275e-08,\n",
       "        1.10562806e-06, 1.16656303e-11],\n",
       "       [4.47134804e-08, 1.24227430e-08, 2.94663154e-07, 3.00637122e-13,\n",
       "        2.58632168e-12, 3.34534977e-07, 2.24281836e-09, 4.46293014e-07,\n",
       "        2.36425378e-08, 3.32133672e-13],\n",
       "       [6.01008130e-03, 1.70907484e-04, 2.36118249e-02, 1.54471169e-08,\n",
       "        3.34534977e-07, 4.46381248e-02, 2.09677475e-05, 5.85150886e-02,\n",
       "        4.09025426e-04, 1.37876962e-08],\n",
       "       [3.49943224e-06, 3.46374235e-02, 5.00618247e-04, 2.76961657e-07,\n",
       "        2.24281836e-09, 2.09677475e-05, 1.33018072e-02, 7.30617790e-05,\n",
       "        3.16765848e-02, 4.26968523e-07],\n",
       "       [7.82125665e-03, 5.19942089e-04, 4.05127345e-02, 2.84146275e-08,\n",
       "        4.46293014e-07, 5.85150886e-02, 7.30617790e-05, 7.76625615e-02,\n",
       "        1.15944498e-03, 2.71846575e-08],\n",
       "       [6.05666037e-05, 1.29322006e-01, 1.36342837e-02, 1.10562806e-06,\n",
       "        2.36425378e-08, 4.09025426e-04, 3.16765848e-02, 1.15944498e-03,\n",
       "        1.78335968e-01, 1.61387413e-06],\n",
       "       [1.81567508e-09, 1.37987206e-06, 2.25756872e-07, 1.16656303e-11,\n",
       "        3.32133672e-13, 1.37876962e-08, 4.26968523e-07, 2.71846575e-08,\n",
       "        1.61387413e-06, 1.73632581e-11]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7141312609510505e-63"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_m = np.linalg.det(M)\n",
    "det_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.18680768e-02],\n",
       "       [3.98095290e-01],\n",
       "       [8.21019731e-01],\n",
       "       [4.17406086e-06],\n",
       "       [2.31103579e-06],\n",
       "       [2.47437674e-01],\n",
       "       [1.00914268e-01],\n",
       "       [3.61869757e-01],\n",
       "       [5.66074568e-01],\n",
       "       [5.77661957e-06]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b_matrix(b_neural_net,sigma,weights, biases,f,neurons)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.16463341e+02],\n",
       "       [ 4.23484024e+01],\n",
       "       [-4.62908029e-01],\n",
       "       [ 4.94178548e+07],\n",
       "       [-9.30213113e+06],\n",
       "       [-1.04493794e+02],\n",
       "       [ 1.83037329e+02],\n",
       "       [ 9.10911228e+01],\n",
       "       [ 1.42404757e+01],\n",
       "       [-4.19783172e+07]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.linalg.solve(M,b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5294/1981966750.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u[j] += single_neural_net(x[j], weights, biases, i, sigma)*a[i]\n"
     ]
    }
   ],
   "source": [
    "collocation = 10\n",
    "u = u_nn(x,weights,biases,sin_function,collocation,neurons,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13505607.12224047, 13703751.40247765, 13892622.30898013,\n",
       "       14072439.92721346, 14243430.12223593, 14405824.05157028,\n",
       "       14559857.67527023, 14705771.26383874, 14843808.90465736,\n",
       "       14974218.00758182])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Loss: 2044450187260788.2\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0,1,collocation)\n",
    "loss = l2_loss(f(x), u)\n",
    "print(\"L2 Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6818c013192d840514133cc15ea0bc15bb95d666290febd40d542c4e37363947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
